{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fp16util import *\n",
    "from resnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import Learner, TrainingPhase, ModelData, accuracy, DecayType\n",
    "from functools import partial\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.models.cifar10.wideresnet import wrn_22_cat, wrn_22, WideResNetConcat\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path.home()/'data/cifar10/'\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlock(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "        \n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoaugment import CIFAR10Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def torch_loader(data_path, size, bs, val_bs=None):\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    # Data loading code\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703,0.24349,0.26159))]\n",
    "\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(), CIFAR10Policy(), \n",
    "    ] + tfms)\n",
    "    val_tfms = transforms.Compose(tfms)\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=train_tfms)\n",
    "    val_dataset  = datasets.CIFAR10(root=data_path, train=False, download=True, transform=val_tfms)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "    \n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    data = ModelData(data_path, train_loader, val_loader)\n",
    "    data.sz = size\n",
    "    return data\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305519a9d53449d99e9d8a023fb4a228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.764004   1.669598   0.4276    \n",
      "    1      1.287702   1.376742   0.5179                     \n",
      "    2      1.037439   1.514856   0.5442                     \n",
      "    3      0.905836   0.901278   0.6948                      \n",
      "    4      0.828128   0.779816   0.7373                      \n",
      "    5      0.77735    1.133747   0.677                       \n",
      "    6      0.728096   0.632654   0.7776                      \n",
      "    7      0.677027   0.73582    0.7738                      \n",
      "    8      0.634679   0.575511   0.8066                      \n",
      "    9      0.612155   0.579327   0.8046                      \n",
      "    10     0.581803   0.48661    0.8336                      \n",
      "    11     0.559674   0.475198   0.8409                      \n",
      "    12     0.552187   0.534952   0.8272                      \n",
      "    13     0.539548   0.605655   0.8049                      \n",
      "    14     0.543335   0.538854   0.8243                      \n",
      "    15     0.507774   0.470604   0.8436                      \n",
      "    16     0.486774   0.431227   0.8567                      \n",
      "    17     0.458291   0.410518   0.8657                      \n",
      "    18     0.443815   0.353423   0.8801                      \n",
      "    19     0.415729   0.339512   0.8861                      \n",
      "    20     0.395578   0.370179   0.8829                      \n",
      "    21     0.362068   0.313982   0.8975                      \n",
      "    22     0.351529   0.287877   0.9007                      \n",
      "    23     0.332159   0.268831   0.9077                      \n",
      "    24     0.309337   0.267368   0.9134                      \n",
      "    25     0.296604   0.301751   0.9015                      \n",
      "    26     0.27979    0.236216   0.9215                      \n",
      "    27     0.259608   0.231502   0.9229                      \n",
      "    28     0.23947    0.212738   0.9287                      \n",
      "    29     0.226911   0.201785   0.9303                      \n",
      "    30     0.212679   0.196374   0.9332                      \n",
      "    31     0.209788   0.199097   0.9336                      \n",
      "    32     0.196254   0.197407   0.9338                      \n",
      "    33     0.211124   0.196611   0.9336                      \n",
      "    34     0.203698   0.195782   0.9331                      \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.195781640625, 0.9330999994277954]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=1e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 256\n",
    "lrs = (0, 2e-1, 1e-2, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "# learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd, 'momentum':0.9}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d5f09229ac47fa924d10f449f4c633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.896388   1.69485    0.3945    \n",
      "    1      1.489245   1.427309   0.5192                   \n",
      "    2      1.17452    1.538377   0.5762                   \n",
      "    3      0.980957   0.925872   0.6939                    \n",
      "    4      0.871169   0.68867    0.7655                    \n",
      "    5      0.795293   0.770804   0.7301                    \n",
      "    6      0.747305   0.606716   0.7926                    \n",
      "    7      0.699317   0.74764    0.7397                    \n",
      "    8      0.664783   0.593196   0.8032                    \n",
      "    9      0.649147   1.022948   0.6906                    \n",
      "    10     0.644033   0.675278   0.7738                    \n",
      "    11     0.626524   0.685498   0.777                     \n",
      "    12     0.626378   0.682255   0.7753                    \n",
      "    13     0.643646   0.545291   0.8178                    \n",
      "    14     0.627443   0.630255   0.7754                    \n",
      "    15     0.630527   0.655323   0.7877                    \n",
      "    16     0.594266   0.629752   0.7748                    \n",
      "    17     0.569922   0.676956   0.7687                    \n",
      "    18     0.545385   0.81944    0.7352                    \n",
      "    19     0.51212    0.492825   0.839                     \n",
      "    20     0.498713   0.697024   0.7789                    \n",
      "    21     0.47446    0.389071   0.8657                    \n",
      "    22     0.451502   0.417259   0.8551                    \n",
      "    23     0.431282   0.591982   0.8094                    \n",
      "    24     0.402947   0.35175    0.8776                    \n",
      "    25     0.377867   0.336735   0.8873                    \n",
      "    26     0.354232   0.309371   0.8988                    \n",
      "    27     0.326706   0.2652     0.9102                    \n",
      "    28     0.293898   0.239068   0.921                     \n",
      "    29     0.257946   0.203742   0.9288                    \n",
      "    30     0.239758   0.201383   0.9315                    \n",
      "    31     0.229457   0.197283   0.9301                    \n",
      " 99%|█████████▉| 97/98 [00:06<00:00, 15.41it/s, loss=0.22] "
     ]
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=5e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 512\n",
    "lrs = (0, 4e-1, 2e-2, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "# learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd, 'momentum':0.9}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUpDataLoader(object):\n",
    "    \"\"\"\n",
    "    Creates a new data loader with mixup from a given dataloader.\n",
    "    \n",
    "    Mixup is applied between a batch and a shuffled version of itself. \n",
    "    If we use a regular beta distribution, this can create near duplicates as some lines might be \n",
    "    1 * original + 0 * shuffled while others could be 0 * original + 1 * shuffled, this is why\n",
    "    there is a trick where we take the maximum of lambda and 1-lambda.\n",
    "    \n",
    "    Arguments:\n",
    "    dl (DataLoader): the data loader to mix up\n",
    "    alpha (float): value of the parameter to use in the beta distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dl, alpha):\n",
    "        self.dl, self.alpha = dl, alpha\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for (x, y) in iter(self.dl):\n",
    "            #Taking one different lambda per image speeds up training \n",
    "            lambd = np.random.beta(self.alpha, self.alpha, y.size(0))\n",
    "            #Trick to avoid near duplicates\n",
    "            lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n",
    "            lambd = to_gpu(VV(lambd)).half()\n",
    "            shuffle = torch.randperm(y.size(0))\n",
    "            x1, y1 = x[shuffle], y[shuffle]\n",
    "            yield (x.half() * lambd.view(lambd.size(0),1,1,1) + x1.half() * (1-lambd).view(lambd.size(0),1,1,1), [y, y1, lambd])\n",
    "            \n",
    "class MixUpLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapts the loss function to go with mixup.\n",
    "    \n",
    "    Since the targets aren't one-hot encoded, we use the linearity of the loss function with\n",
    "    regards to the target to mix up the loss instead of one-hot encoded targets.\n",
    "    \n",
    "    Argument:\n",
    "    crit: a loss function. It must have the parameter reduced=False to have the loss per element.\n",
    "    \"\"\"\n",
    "    def __init__(self, crit):\n",
    "        super().__init__()\n",
    "        self.crit = crit()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        if not isinstance(target, list): return self.crit(output, target).mean()\n",
    "        loss1, loss2 = self.crit(output,target[0]), self.crit(output,target[1])\n",
    "        return (loss1 * target[2] + loss2 * (1-target[2])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = wrn_22()\n",
    "# opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "# learn = ConvLearner.from_model_data(m, data)\n",
    "# learn.metrics = [accuracy]\n",
    "# wd=1e-4\n",
    "# learn.opt_fn = opt_fn\n",
    "# learn.data.trn_dl = mixup_dl\n",
    "# learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82be1a8ef334fdd8182cbfa74e39c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      2.014805   1.654992   0.394     \n",
      "    1      1.780525   1.236267   0.5818                     \n",
      "    2      1.641302   1.180612   0.6062                     \n",
      "    3      1.573484   1.190717   0.605                      \n",
      "    4      1.50012    1.000758   0.6753                     \n",
      "    5      1.438768   0.765143   0.747                      \n",
      "    6      1.413901   0.72423    0.771                      \n",
      "    7      1.378722   0.798111   0.7356                     \n",
      "    8      1.351556   0.68767    0.793                      \n",
      "    9      1.317362   0.704825   0.7809                     \n",
      "    10     1.30258    0.605433   0.8158                     \n",
      "    11     1.283154   0.653912   0.7936                     \n",
      "    12     1.276553   0.575738   0.8386                     \n",
      "    13     1.267437   0.547024   0.8428                     \n",
      "    14     1.255462   0.494032   0.8588                     \n",
      "    15     1.240891   0.48393    0.8655                     \n",
      "    16     1.207685   0.486838   0.8575                     \n",
      "    17     1.19267    0.447324   0.8736                     \n",
      "    18     1.17751    0.478236   0.8616                     \n",
      "    19     1.157905   0.388672   0.8856                     \n",
      "    20     1.138311   0.420434   0.8838                     \n",
      "    21     1.135491   0.392963   0.8969                     \n",
      "    22     1.124893   0.399493   0.8911                     \n",
      "    23     1.104853   0.341336   0.9057                     \n",
      "    24     1.101226   0.338377   0.9121                     \n",
      "    25     1.083762   0.330679   0.9137                     \n",
      "    26     1.060888   0.338098   0.9123                     \n",
      "    27     1.062479   0.309853   0.9226                     \n",
      "    28     1.048099   0.307969   0.9223                     \n",
      "    29     1.042165   0.302483   0.9226                     \n",
      "    30     1.030305   0.30067    0.924                      \n",
      "    31     1.030809   0.298186   0.9251                     \n",
      "    32     1.028659   0.295651   0.9239                     \n",
      "    33     1.026751   0.296578   0.9255                     \n",
      "    34     1.023768   0.296801   0.9258                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29680078125, 0.925799999332428]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=1e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 256\n",
    "lrs = (0, 2e-1, 1e-2, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "mixup_dl = MixUpDataLoader(data.trn_dl, 0.6)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "# learn.half()\n",
    "learn.data.trn_dl = mixup_dl\n",
    "learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "\n",
    "# opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd, 'momentum':0.9}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936df25085994eeba5ad638b797dff3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      2.090858   1.562464   0.4282    \n",
      "    1      1.867956   1.38808    0.5056                   \n",
      "    2      1.699242   1.119784   0.615                    \n",
      "    3      1.585528   0.949446   0.6759                   \n",
      "    4      1.510021   0.903056   0.6909                   \n",
      "    5      1.450629   0.806435   0.7549                   \n",
      "    6      1.417132   0.739396   0.7768                   \n",
      "    7      1.385784   0.741787   0.7661                   \n",
      "    8      1.355923   0.739975   0.7696                   \n",
      "    9      1.337761   0.613872   0.8132                   \n",
      "    10     1.329115   0.696055   0.7866                   \n",
      "    11     1.307615   0.60846    0.8377                   \n",
      "    12     1.302465   0.725997   0.7926                   \n",
      "    13     1.287697   0.580276   0.8339                   \n",
      "    14     1.279528   0.604704   0.823                    \n",
      "    15     1.249727   0.841876   0.7368                   \n",
      "    16     1.243811   0.799248   0.7708                   \n",
      "    17     1.215235   0.624316   0.828                    \n",
      "    18     1.198161   0.62389    0.8637                   \n",
      "    19     1.184721   0.60205    0.8546                   \n",
      "    20     1.16243    0.539229   0.8619                   \n",
      "    21     1.145376   0.56117    0.8586                   \n",
      "    22     1.137319   0.615207   0.8512                   \n",
      "    23     1.116048   0.500385   0.877                    \n",
      "    24     1.118199   0.441617   0.8965                   \n",
      "    25     1.101283   0.481966   0.8919                   \n",
      "    26     1.092442   0.471316   0.899                    \n",
      "    27     1.07579    0.403537   0.9041                   \n",
      "    28     1.069493   0.35874    0.9102                   \n",
      "    29     1.053858   0.35848    0.918                    \n",
      "    30     1.045772   0.339862   0.9163                   \n",
      "    31     1.045287   0.332647   0.919                    \n",
      "    32     1.041844   0.316287   0.9202                   \n",
      "    33     1.040675   0.318332   0.9191                   \n",
      "    34     1.038726   0.309226   0.9209                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.30922578125, 0.9208999980926513]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "# model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=1e-5\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 512\n",
    "lrs = (0, 3e-3, 2e-4, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "mixup_dl = MixUpDataLoader(data.trn_dl, 0.6)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "learn.half()\n",
    "learn.data.trn_dl = mixup_dl\n",
    "learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learn.metrics = [accuracy]\n",
    "# learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd, 'momentum':0.9}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "# model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=1e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 256\n",
    "lrs = (0, 2e-3, 1e-4, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "mixup_dl = MixUpDataLoader(data.trn_dl, 0.6)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "learn.half()\n",
    "learn.data.trn_dl = mixup_dl\n",
    "learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learn.metrics = [accuracy]\n",
    "# learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccaa5a75339471ea19772aeac943dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafe548262d14feca3dd9b65ce8661aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      2.034803   1.607208   0.4254    \n",
      "    1      1.791129   1.4785     0.4813                     \n",
      "    2      1.639594   1.141684   0.5963                     \n",
      "    3      1.551836   0.995409   0.661                      \n",
      "    4      1.483802   0.949103   0.6885                     \n",
      "    5      1.438598   0.73587    0.7622                     \n",
      "    6      1.393267   0.776724   0.75                       \n",
      "    7      1.365727   0.69194    0.78                       \n",
      "    8      1.343344   0.662018   0.7902                     \n",
      "    9      1.323702   0.657792   0.7974                     \n",
      "    10     1.312869   0.63128    0.7912                     \n",
      "    11     1.295111   0.626956   0.7985                     \n",
      "    12     1.265577   0.51227    0.8517                     \n",
      "    13     1.269983   0.522676   0.8464                     \n",
      "    14     1.250852   0.53137    0.8413                     \n",
      "    15     1.231268   0.495957   0.8567                     \n",
      "    16     1.206407   0.509833   0.8634                     \n",
      "    17     1.179515   0.440716   0.8669                     \n",
      "    18     1.15429    0.393257   0.8876                     \n",
      "    19     1.147747   0.41182    0.8862                     \n",
      "    20     1.134622   0.360271   0.8954                     \n",
      "    21     1.118328   0.374801   0.896                      \n",
      "    22     1.095706   0.368593   0.9027                     \n",
      "    23     1.085074   0.369369   0.9079                     \n",
      "    24     1.078917   0.339977   0.9107                     \n",
      "    25     1.060197   0.313143   0.9198                     \n",
      "    26     1.051093   0.320136   0.9208                     \n",
      "    27     1.040032   0.302509   0.9226                     \n",
      "    28     1.028761   0.294111   0.9245                     \n",
      "    29     1.021496   0.287578   0.9279                     \n",
      "    30     1.011881   0.284983   0.9277                     \n",
      "    31     1.019166   0.291453   0.9295                     \n",
      "    32     1.008514   0.282049   0.9298                     \n",
      "    33     1.017903   0.281287   0.9309                     \n",
      "    34     1.008291   0.278912   0.9309                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2789125, 0.930899999332428]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441b5cdbfca74cc0a1b2a48be363320a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      2.121836   1.754028   0.352     \n",
      "    1      1.873292   1.372998   0.5145                     \n",
      "    2      1.693561   1.335194   0.5366                     \n",
      "    3      1.576514   1.131819   0.6077                     \n",
      "    4      1.481769   0.9215     0.6833                     \n",
      "    5      1.490514   0.810545   0.7316                     \n",
      "    6      1.48339    0.810693   0.7328                     \n",
      "    7      1.489809   0.807862   0.735                      \n",
      "    8      1.497329   0.81227    0.732                      \n",
      " 51%|█████     | 100/196 [00:04<00:03, 24.30it/s, loss=1.5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1477:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-41eca77635e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_opt_sched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit_opt_sched\u001b[0;34m(self, phases, cycle_save_name, best_save_name, stop_div, data_list, callbacks, cut, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_crit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/model.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcopy_fp32_to_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastai/fp16.py\u001b[0m in \u001b[0;36mcopy_fp32_to_model\u001b[0;34m(m, fp32_params)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfp32_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_param\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mm_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp32_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fp32_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp32_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1475:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1476:\n",
      "Process Process-1473:\n",
      "Process Process-1472:\n",
      "Process Process-1471:\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1474:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/Image.py\", line 429, in _getdecoder\n",
      "    decoder = DECODERS[decoder_name]\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyError: 'raw'\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/datasets/cifar.py\", line 121, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-6-10ee7b0e9c2e>\", line 5, in pad\n",
      "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/lib/arraypad.py\", line 1313, in pad\n",
      "    'wrap': [],\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/cluster/pytorch-cifar/autoaugment.py\", line 111, in __call__\n",
      "    return self.policies[policy_idx](img)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/cluster/pytorch-cifar/autoaugment.py\", line 237, in __call__\n",
      "    if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n",
      "  File \"/home/paperspace/cluster/pytorch-cifar/autoaugment.py\", line 220, in <lambda>\n",
      "    \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/ImageOps.py\", line 130, in autocontrast\n",
      "    ix = int(ix * scale + offset)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n"
     ]
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "# model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=1e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 256\n",
    "lrs = (0, 2e-3, 1e-4, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "data = torch_loader(PATH, sz, bs, bs*2)\n",
    "mixup_dl = MixUpDataLoader(data.trn_dl, 0.6)\n",
    "    \n",
    "learn = Learner.from_model_data(model, data)\n",
    "learn.half()\n",
    "learn.data.trn_dl = mixup_dl\n",
    "learn.crit = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False))\n",
    "learn.metrics = [accuracy]\n",
    "# learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd}\n",
    "phases = [\n",
    "    TrainingPhase(opt_fn=partial(torch.optim.SGD, nesterov=True, momentum=0.9), wds=wd, epochs=5, lr=(0,2e-2), lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "#     TrainingPhase(opt_fn=partial(torch.optim.SGD, nesterov=True, momentum=0.9), wds=wd, epochs=15, lr=(0,2e-1), lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=10, lr=(2e-4,2e-3), lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR, wd_loss=False),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
