{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, os, shutil, time, warnings\n",
    "\n",
    "from fp16util import *\n",
    "from resnet import *\n",
    "from PIL import Image\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--phases', default='[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]', type=str,\n",
    "                    help='Should be a string formatted like this: [(start_lr,end_lr,num_epochs),(phase2...)]')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Verbose logging')\n",
    "#     parser.add_argument('--init-bn0', action='store_true', help='Intialize running batch norm mean to 0')\n",
    "    parser.add_argument('--print-freq', '-p', default=200, type=int,\n",
    "                        metavar='N', help='print every this many steps (default: 5)')\n",
    "#     parser.add_argument('--no-bn-wd', action='store_true', help='Remove batch norm from weight decay')\n",
    "    parser.add_argument('--full-precision', action='store_true', help='Run model full precision mode. Default fp16')\n",
    "    parser.add_argument('--loss-scale', type=float, default=512,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--world-size', default=-1, type=int, \n",
    "                        help='total number of processes (machines*gpus)')\n",
    "    parser.add_argument('--scale-lr', type=float, default=1, help='You should learning rate propotionally to world size')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input = [\n",
    "    str(Path.home()/'data/cifar10/'),\n",
    "    '--phases', '[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]',\n",
    "#     '--phases', '[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]'\n",
    "#     '--full-precision'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args\n",
    "args = get_parser().parse_args(args_input)\n",
    "if args.full_precision: args.loss_scale = 1\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "class PreActBlockBN0(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.bn2.weight = Parameter(torch.zeros_like(self.bn2.weight))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlockBN0(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "        \n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
    "\n",
    "def torch_loader(data_path, size, bs, val_bs=None):\n",
    "    os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    # Data loading code\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703,0.24349,0.26159))]\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tfms)\n",
    "    val_tfms = transforms.Compose(tfms)\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=(args.local_rank==0), transform=train_tfms)\n",
    "    val_dataset  = datasets.CIFAR10(root=data_path, train=False, download=(args.local_rank==0), transform=val_tfms)\n",
    "\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    # val_sampler = (torch.utils.data.distributed.DistributedSampler(val_dataset) if args.distributed else None)\n",
    "    val_sampler = None\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=val_sampler)\n",
    "    \n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, optimizer, phases=[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_lr = None\n",
    "        self.phases = phases\n",
    "        self.tot_epochs = sum([p[2] for p in phases])\n",
    "\n",
    "    def linear_lr(self, start_lr, end_lr, epoch_curr, batch_curr, epoch_tot, batch_tot):\n",
    "        if args.scale_lr != 1:\n",
    "            start_lr *= args.scale_lr\n",
    "            end_lr *= args.scale_lr\n",
    "        step_tot = epoch_tot * batch_tot\n",
    "        step_curr = epoch_curr * batch_tot + batch_curr\n",
    "        step_size = (end_lr - start_lr)/step_tot\n",
    "        return start_lr + step_curr * step_size\n",
    "    \n",
    "    def get_current_phase(self, epoch):\n",
    "        epoch_accum = 0\n",
    "        for phase in self.phases:\n",
    "            start_lr,end_lr,num_epochs = phase\n",
    "            if epoch <= epoch_accum+num_epochs: return start_lr, end_lr, num_epochs, epoch - epoch_accum\n",
    "            epoch_accum += num_epochs\n",
    "        raise Exception('Epoch out of range')\n",
    "            \n",
    "    def get_lr(self, epoch, batch_curr, batch_tot):\n",
    "        start_lr, end_lr, num_epochs, relative_epoch = self.get_current_phase(epoch)\n",
    "        return self.linear_lr(start_lr, end_lr, relative_epoch, batch_curr, num_epochs, batch_tot)\n",
    "\n",
    "    def update_lr(self, epoch, batch_num, batch_tot):\n",
    "        lr = self.get_lr(epoch, batch_num, batch_tot)\n",
    "        if args.verbose and (self.current_lr != lr) and ((batch_num == 1) or (batch_num == batch_tot)): \n",
    "            print(f'Changing LR from {self.current_lr} to {lr}')\n",
    "\n",
    "        self.current_lr = lr\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            lr_old = param_group['lr'] or lr\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if isinstance(t, float): return t\n",
    "    if isinstance(t, int): return t\n",
    "    if hasattr(t, 'item'): return t.item()\n",
    "    else: return t[0]\n",
    "\n",
    "def train(trn_loader, model, criterion, optimizer, scheduler, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    st = time.time()\n",
    "    trn_len = len(trn_loader)\n",
    "\n",
    "    # print('Begin training loop:', st)\n",
    "    for i,(input,target) in enumerate(trn_loader):\n",
    "        batch_size = input.size(0)\n",
    "        batch_num = i+1\n",
    "        \n",
    "        # measure data loading time\n",
    "        scheduler.update_lr(epoch, i+1, trn_len)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        if args.distributed:\n",
    "            # Must keep track of global batch size, since not all machines are guaranteed equal batches at the end of an epoch\n",
    "            corr1 = correct(output.data, target)[0]\n",
    "            metrics = torch.tensor([batch_size, loss, corr1]).float().cuda()\n",
    "            batch_total, reduced_loss, corr1 = sum_tensor(metrics)\n",
    "            reduced_loss = reduced_loss/dist.get_world_size()\n",
    "            prec1 = corr1*(100.0/batch_total)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            batch_total = input.size(0)\n",
    "            prec1 = accuracy(output.data, target)[0] # measure accuracy and record loss\n",
    "        losses.update(to_python_float(reduced_loss), to_python_float(batch_total))\n",
    "        top1.update(to_python_float(prec1), to_python_float(batch_total))\n",
    "\n",
    "        loss = loss*args.loss_scale\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        if args.full_precision:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            model_grads_to_master_grads(model_params, master_params)\n",
    "            for param in master_params:\n",
    "                param.grad.data = param.grad.data/args.loss_scale\n",
    "            optimizer.step()\n",
    "            master_params_to_model_params(model_params, master_params)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==trn_len)\n",
    "        if should_print: log_batch(epoch, batch_num, trn_len, batch_time, losses, top1)\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    val_len = len(val_loader)\n",
    "\n",
    "    for i,(input,target) in enumerate(val_loader):\n",
    "        batch_num = i+1\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target).data\n",
    "        batch_total = input.size(0)\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "            \n",
    "        losses.update(to_python_float(loss), batch_total)\n",
    "        top1.update(to_python_float(prec1), batch_total)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==val_len)\n",
    "        if should_print: log_batch(epoch, batch_num, val_len, batch_time, losses, top1)\n",
    "            \n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def log_batch(epoch, batch_num, batch_len, batch_time, loss, top1):\n",
    "    if args.local_rank==0 and args.verbose:\n",
    "        output = ('Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})').format(\n",
    "                epoch, batch_num, batch_len, batch_time=batch_time, loss=loss, top1=top1)\n",
    "        print(output)\n",
    "        with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "            f.write(output + '\\n')\n",
    "            \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = self.avg = self.sum = self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    corrrect_ks = correct(output, target, topk)\n",
    "    batch_size = target.size(0)\n",
    "    return [correct_k.float().mul_(100.0 / batch_size) for correct_k in corrrect_ks]\n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sum_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    return rt\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=True, local_rank=0, loss_scale=1, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0005, workers=8, world_size=-1)\n",
      "\n",
      "\n",
      "\n",
      "epoch\t\tnum_batch\ttime (min)\ttrn_loss\tval_loss\taccuracy\n",
      "0\t\t196\t\t0.1384\t\t2.089\t\t1.853\t\t32.16\n",
      "1\t\t196\t\t0.2612\t\t1.7702\t\t1.7129\t\t37.63\n",
      "2\t\t196\t\t0.3836\t\t1.6473\t\t1.6338\t\t41.11\n",
      "3\t\t196\t\t0.5049\t\t1.5818\t\t1.5732\t\t42.18\n",
      "4\t\t196\t\t0.629\t\t1.5464\t\t1.8002\t\t36.59\n",
      "5\t\t196\t\t0.7505\t\t1.5218\t\t1.6019\t\t43.07\n",
      "6\t\t196\t\t0.8733\t\t1.4975\t\t1.5097\t\t45.68\n",
      "7\t\t196\t\t0.9951\t\t1.4852\t\t1.5172\t\t45.8\n",
      "8\t\t196\t\t1.1175\t\t1.478\t\t1.4822\t\t46.72\n",
      "9\t\t196\t\t1.2392\t\t1.4645\t\t1.6509\t\t40.78\n",
      "10\t\t196\t\t1.3625\t\t1.4647\t\t1.5578\t\t43.91\n",
      "11\t\t196\t\t1.4852\t\t1.4559\t\t1.5938\t\t42.41\n",
      "12\t\t196\t\t1.6086\t\t1.4599\t\t1.4767\t\t47.07\n",
      "13\t\t196\t\t1.7318\t\t1.4605\t\t1.7348\t\t37.48\n",
      "14\t\t196\t\t1.8547\t\t1.4647\t\t1.601\t\t42.22\n",
      "15\t\t196\t\t1.9769\t\t1.4693\t\t1.5358\t\t44.9\n",
      "16\t\t196\t\t2.1003\t\t1.4419\t\t1.5047\t\t45.66\n",
      "17\t\t196\t\t2.222\t\t1.4291\t\t1.4531\t\t48.18\n",
      "18\t\t196\t\t2.3439\t\t1.4161\t\t1.4195\t\t49.13\n",
      "19\t\t196\t\t2.466\t\t1.3908\t\t1.3994\t\t49.9\n",
      "20\t\t196\t\t2.5895\t\t1.3837\t\t1.3953\t\t49.59\n",
      "21\t\t196\t\t2.7112\t\t1.3731\t\t1.3671\t\t50.46\n",
      "22\t\t196\t\t2.8343\t\t1.3561\t\t1.3917\t\t50.08\n",
      "23\t\t196\t\t2.957\t\t1.3395\t\t1.335\t\t51.85\n",
      "24\t\t196\t\t3.079\t\t1.3265\t\t1.3537\t\t51.53\n",
      "25\t\t196\t\t3.2016\t\t1.3092\t\t1.2802\t\t54.25\n",
      "26\t\t196\t\t3.3244\t\t1.2861\t\t1.2491\t\t55.65\n",
      "27\t\t196\t\t3.4468\t\t1.2687\t\t1.2371\t\t55.97\n",
      "28\t\t196\t\t3.5683\t\t1.2407\t\t1.2105\t\t56.9\n",
      "29\t\t196\t\t3.6913\t\t1.2133\t\t1.1912\t\t57.35\n",
      "30\t\t196\t\t3.8146\t\t1.1987\t\t1.738\t\t40.01\n",
      "31\t\t196\t\t3.9361\t\t1.2115\t\t1.1762\t\t58.1\n",
      "32\t\t196\t\t4.0596\t\t1.1788\t\t1.1688\t\t58.44\n",
      "33\t\t196\t\t4.182\t\t1.1723\t\t1.1639\t\t58.76\n",
      "34\t\t196\t\t4.3048\t\t1.1699\t\t1.1621\t\t58.78\n"
     ]
    }
   ],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "if not args.full_precision: model = network_to_half(model)\n",
    "if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# TESTING\n",
    "args.full_precision = True\n",
    "args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "global model_params, master_params\n",
    "if args.full_precision: master_params = model.parameters()\n",
    "else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.SGD(master_params, lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "print(args)\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "    trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
