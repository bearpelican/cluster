{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, os, shutil, time, warnings\n",
    "\n",
    "from fp16util import *\n",
    "from resnet import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--phases', default='[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]', type=str,\n",
    "                    help='Should be a string formatted like this: [(start_lr,end_lr,num_epochs),(phase2...)]')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Verbose logging')\n",
    "#     parser.add_argument('--init-bn0', action='store_true', help='Intialize running batch norm mean to 0')\n",
    "    parser.add_argument('--print-freq', '-p', default=200, type=int,\n",
    "                        metavar='N', help='print every this many steps (default: 5)')\n",
    "#     parser.add_argument('--no-bn-wd', action='store_true', help='Remove batch norm from weight decay')\n",
    "    parser.add_argument('--full-precision', action='store_true', help='Run model full precision mode. Default fp16')\n",
    "    parser.add_argument('--loss-scale', type=float, default=512,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--world-size', default=-1, type=int, \n",
    "                        help='total number of processes (machines*gpus)')\n",
    "    parser.add_argument('--scale-lr', type=float, default=1, help='You should learning rate propotionally to world size')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input = [\n",
    "    str(Path.home()/'data/cifar10/'),\n",
    "    '--phases', '[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]',\n",
    "#     '--phases', '[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]'\n",
    "#     '--full-precision'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args\n",
    "args = get_parser().parse_args(args_input)\n",
    "if args.full_precision: args.loss_scale = 1\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlock(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "        \n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
    "\n",
    "def torch_loader(data_path, size, bs, val_bs=None):\n",
    "    data_path = Path(data_path)\n",
    "    os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    # Data loading code\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703,0.24349,0.26159))]\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tfms)\n",
    "    val_tfms = transforms.Compose(tfms)\n",
    "\n",
    "    download = (args.local_rank==0) and not (data_path/'train').exists()\n",
    "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=download, transform=train_tfms)\n",
    "    val_dataset  = datasets.CIFAR10(root=data_path, train=False, download=download, transform=val_tfms)\n",
    "\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    # val_sampler = (torch.utils.data.distributed.DistributedSampler(val_dataset) if args.distributed else None)\n",
    "    val_sampler = None\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=val_sampler)\n",
    "    \n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, optimizer, phases=[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_lr = None\n",
    "        self.phases = phases\n",
    "        self.tot_epochs = sum([p[2] for p in phases])\n",
    "\n",
    "    def linear_lr(self, start_lr, end_lr, epoch_curr, batch_curr, epoch_tot, batch_tot):\n",
    "        if args.scale_lr != 1:\n",
    "            start_lr *= args.scale_lr\n",
    "            end_lr *= args.scale_lr\n",
    "        step_tot = epoch_tot * batch_tot\n",
    "        step_curr = epoch_curr * batch_tot + batch_curr\n",
    "        step_size = (end_lr - start_lr)/step_tot\n",
    "        return start_lr + step_curr * step_size\n",
    "    \n",
    "    def get_current_phase(self, epoch):\n",
    "        epoch_accum = 0\n",
    "        for phase in self.phases:\n",
    "            start_lr,end_lr,num_epochs = phase\n",
    "            if epoch <= epoch_accum+num_epochs: return start_lr, end_lr, num_epochs, epoch - epoch_accum\n",
    "            epoch_accum += num_epochs\n",
    "        raise Exception('Epoch out of range')\n",
    "            \n",
    "    def get_lr(self, epoch, batch_curr, batch_tot):\n",
    "        start_lr, end_lr, num_epochs, relative_epoch = self.get_current_phase(epoch)\n",
    "        return self.linear_lr(start_lr, end_lr, relative_epoch, batch_curr, num_epochs, batch_tot)\n",
    "\n",
    "    def update_lr(self, epoch, batch_num, batch_tot):\n",
    "        lr = self.get_lr(epoch, batch_num, batch_tot)\n",
    "        if args.verbose and (self.current_lr != lr) and ((batch_num == 1) or (batch_num == batch_tot)): \n",
    "            print(f'Changing LR from {self.current_lr} to {lr}')\n",
    "\n",
    "        self.current_lr = lr\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            lr_old = param_group['lr'] or lr\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if isinstance(t, float): return t\n",
    "    if isinstance(t, int): return t\n",
    "    if hasattr(t, 'item'): return t.item()\n",
    "    else: return t[0]\n",
    "\n",
    "def train(trn_loader, model, criterion, optimizer, scheduler, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    st = time.time()\n",
    "    trn_len = len(trn_loader)\n",
    "\n",
    "    # print('Begin training loop:', st)\n",
    "    for i,(input,target) in enumerate(trn_loader):\n",
    "        batch_size = input.size(0)\n",
    "        batch_num = i+1\n",
    "        \n",
    "        # measure data loading time\n",
    "        scheduler.update_lr(epoch, i, trn_len)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        if args.distributed:\n",
    "            # Must keep track of global batch size, since not all machines are guaranteed equal batches at the end of an epoch\n",
    "            corr1 = correct(output.data, target)[0]\n",
    "            metrics = torch.tensor([batch_size, loss, corr1]).float().cuda()\n",
    "            batch_total, reduced_loss, corr1 = sum_tensor(metrics)\n",
    "            reduced_loss = reduced_loss/dist.get_world_size()\n",
    "            prec1 = corr1*(100.0/batch_total)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            batch_total = input.size(0)\n",
    "#             prec1 = accuracy(output.data, target)[0] # measure accuracy and record loss\n",
    "        losses.update(to_python_float(reduced_loss), to_python_float(batch_total))\n",
    "#         top1.update(to_python_float(prec1), to_python_float(batch_total))\n",
    "\n",
    "#         loss = loss*args.loss_scale\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "#         if args.full_precision:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         else:\n",
    "#             model.zero_grad()\n",
    "#             loss.backward()\n",
    "#             model_grads_to_master_grads(model_params, master_params)\n",
    "#             for param in master_params:\n",
    "#                 param.grad.data = param.grad.data/args.loss_scale\n",
    "#             optimizer.step()\n",
    "#             master_params_to_model_params(model_params, master_params)\n",
    "#             torch.cuda.synchronize()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==trn_len)\n",
    "        if should_print: log_batch(epoch, batch_num, trn_len, batch_time, losses, top1)\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    val_len = len(val_loader)\n",
    "\n",
    "    for i,(input,target) in enumerate(val_loader):\n",
    "        batch_num = i+1\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target).data\n",
    "        batch_total = input.size(0)\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "            \n",
    "        losses.update(to_python_float(loss), batch_total)\n",
    "        top1.update(to_python_float(prec1), batch_total)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==val_len)\n",
    "        if should_print: log_batch(epoch, batch_num, val_len, batch_time, losses, top1)\n",
    "            \n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def log_batch(epoch, batch_num, batch_len, batch_time, loss, top1):\n",
    "    if args.local_rank==0 and args.verbose:\n",
    "        output = ('Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})').format(\n",
    "                epoch, batch_num, batch_len, batch_time=batch_time, loss=loss, top1=top1)\n",
    "        print(output)\n",
    "        with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "            f.write(output + '\\n')\n",
    "            \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = self.avg = self.sum = self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    corrrect_ks = correct(output, target, topk)\n",
    "    batch_size = target.size(0)\n",
    "    return [correct_k.float().mul_(100.0 / batch_size) for correct_k in corrrect_ks]\n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sum_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    return rt\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=False, local_rank=0, loss_scale=512, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0005, workers=8, world_size=-1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import partial\n",
    "class MixUpDataLoader(object):\n",
    "    \"\"\"\n",
    "    Creates a new data loader with mixup from a given dataloader.\n",
    "    \n",
    "    Mixup is applied between a batch and a shuffled version of itself. \n",
    "    If we use a regular beta distribution, this can create near duplicates as some lines might be \n",
    "    1 * original + 0 * shuffled while others could be 0 * original + 1 * shuffled, this is why\n",
    "    there is a trick where we take the maximum of lambda and 1-lambda.\n",
    "    \n",
    "    Arguments:\n",
    "    dl (DataLoader): the data loader to mix up\n",
    "    alpha (float): value of the parameter to use in the beta distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, dl, alpha):\n",
    "        self.dl, self.alpha = dl, alpha\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for (x, y) in iter(self.dl):\n",
    "            #Taking one different lambda per image speeds up training \n",
    "            lambd = np.random.beta(self.alpha, self.alpha, y.size(0))\n",
    "            #Trick to avoid near duplicates\n",
    "            lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)\n",
    "#             lambd = to_gpu(VV(lambd))\n",
    "            lambd = torch.from_numpy(lambd).cuda().float()\n",
    "            shuffle = torch.randperm(y.size(0))\n",
    "            x = x.cuda().half()\n",
    "            x1, y1 = x[shuffle], y[shuffle]\n",
    "            lamd_rs = lambd.view(lambd.size(0),1,1,1).half()\n",
    "            # lambd = lambd.view(lambd.size(0),1,1,1).half()\n",
    "            new_x = x * lamd_rs + x1 * (1-lamd_rs)\n",
    "            yield (new_x, [y, y1, lambd.half()])\n",
    "\n",
    "class MixUpLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapts the loss function to go with mixup.\n",
    "    \n",
    "    Since the targets aren't one-hot encoded, we use the linearity of the loss function with\n",
    "    regards to the target to mix up the loss instead of one-hot encoded targets.\n",
    "    \n",
    "    Argument:\n",
    "    crit: a loss function. It must have the parameter reduced=False to have the loss per element.\n",
    "    \"\"\"\n",
    "    def __init__(self, crit):\n",
    "        super().__init__()\n",
    "        self.crit = crit()\n",
    "        \n",
    "    def forward(self, output, target):\n",
    "        if not isinstance(target, list): return self.crit(output, target).mean()\n",
    "        loss1, loss2 = self.crit(output,target[0]), self.crit(output,target[1])\n",
    "        return (loss1 * target[2] + loss2 * (1-target[2])).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "model = network_to_half(model)\n",
    "\n",
    "# args.full_precision = False\n",
    "# if not args.full_precision: model = network_to_half(model)\n",
    "# if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# # TESTING\n",
    "# args.full_precision = True\n",
    "# args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "# global model_params, master_params\n",
    "# if args.full_precision: master_params = model.parameters()\n",
    "# else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "# criterion = F.cross_entropy\n",
    "criterion = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False)).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "mixup_dl = MixUpDataLoader(trn_loader, 0.6)\n",
    "\n",
    "print(args)\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "#     trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    trn_top1, trn_loss = train(mixup_dl, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=False, local_rank=0, loss_scale=512, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0005, workers=8, world_size=-1)\n",
      "\n",
      "\n",
      "\n",
      "epoch\t\tnum_batch\ttime (min)\ttrn_loss\tval_loss\taccuracy\n",
      "0\t\t196\t\t0.1419\t\t1.9716\t\t1.5445\t\t45.22\n",
      "1\t\t196\t\t0.2663\t\t1.5729\t\t1.5425\t\t46.03\n",
      "2\t\t196\t\t0.391\t\t1.3999\t\t0.9298\t\t68.91\n",
      "3\t\t196\t\t0.5174\t\t1.3022\t\t0.9513\t\t67.89\n",
      "4\t\t196\t\t0.6421\t\t1.2431\t\t0.8092\t\t72.92\n",
      "5\t\t196\t\t0.7679\t\t1.206\t\t0.9283\t\t67.85\n",
      "6\t\t196\t\t0.892\t\t1.1771\t\t0.8694\t\t72.82\n",
      "7\t\t196\t\t1.0176\t\t1.1435\t\t0.6656\t\t80.97\n",
      "8\t\t196\t\t1.1428\t\t1.131\t\t0.7744\t\t75.15\n",
      "9\t\t196\t\t1.2696\t\t1.1117\t\t0.7833\t\t74.8\n",
      "10\t\t196\t\t1.3944\t\t1.1009\t\t0.7347\t\t76.53\n",
      "11\t\t196\t\t1.5192\t\t1.0967\t\t0.6273\t\t80.97\n",
      "12\t\t196\t\t1.645\t\t1.0923\t\t0.7852\t\t74.65\n",
      "13\t\t196\t\t1.7699\t\t1.0884\t\t1.0566\t\t64.79\n",
      "14\t\t196\t\t1.8951\t\t1.0905\t\t0.9272\t\t71.54\n",
      "15\t\t196\t\t2.0214\t\t1.0894\t\t0.637\t\t80.18\n",
      "16\t\t196\t\t2.1477\t\t1.0637\t\t0.6583\t\t78.46\n",
      "17\t\t196\t\t2.272\t\t1.0482\t\t0.6337\t\t79.51\n",
      "18\t\t196\t\t2.3977\t\t1.0328\t\t0.607\t\t81.73\n",
      "19\t\t196\t\t2.5229\t\t1.0214\t\t0.4979\t\t85.73\n",
      "20\t\t196\t\t2.6472\t\t1.0068\t\t0.5835\t\t81.39\n",
      "21\t\t196\t\t2.7724\t\t0.9918\t\t0.6233\t\t80.46\n",
      "22\t\t196\t\t2.8971\t\t0.9733\t\t0.4633\t\t86.69\n",
      "23\t\t196\t\t3.0226\t\t0.9627\t\t0.5429\t\t83.45\n",
      "24\t\t196\t\t3.1485\t\t0.9444\t\t0.3902\t\t88.78\n",
      "25\t\t196\t\t3.272\t\t0.9286\t\t0.3883\t\t88.88\n",
      "26\t\t196\t\t3.3971\t\t0.9066\t\t0.3423\t\t90.36\n",
      "27\t\t196\t\t3.5216\t\t0.8842\t\t0.311\t\t91.49\n",
      "28\t\t196\t\t3.646\t\t0.8648\t\t0.2831\t\t92.63\n",
      "29\t\t196\t\t3.7711\t\t0.8372\t\t0.2551\t\t93.44\n",
      "30\t\t196\t\t3.8966\t\t0.8257\t\t0.2452\t\t93.83\n",
      "31\t\t196\t\t4.0218\t\t0.8187\t\t0.2474\t\t93.75\n",
      "32\t\t196\t\t4.1472\t\t0.8086\t\t0.2376\t\t93.64\n",
      "33\t\t196\t\t4.271\t\t0.8077\t\t0.2364\t\t93.86\n",
      "34\t\t196\t\t4.3962\t\t0.8015\t\t0.2374\t\t93.88\n"
     ]
    }
   ],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "model = network_to_half(model)\n",
    "\n",
    "# args.full_precision = False\n",
    "# if not args.full_precision: model = network_to_half(model)\n",
    "# if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# # TESTING\n",
    "# args.full_precision = True\n",
    "# args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "# global model_params, master_params\n",
    "# if args.full_precision: master_params = model.parameters()\n",
    "# else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "# criterion = F.cross_entropy\n",
    "criterion = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False)).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "mixup_dl = MixUpDataLoader(trn_loader, 0.6)\n",
    "\n",
    "print(args)\n",
    "args.weight_decay = 1e-4\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "#     trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    trn_top1, trn_loss = train(mixup_dl, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=False, local_rank=0, loss_scale=512, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0001, workers=8, world_size=-1)\n",
      "\n",
      "\n",
      "\n",
      "epoch\t\tnum_batch\ttime (min)\ttrn_loss\tval_loss\taccuracy\n",
      "0\t\t196\t\t0.1247\t\t2.0259\t\t1.8075\t\t32.18\n",
      "1\t\t196\t\t0.2509\t\t1.6535\t\t1.5381\t\t46.03\n",
      "2\t\t196\t\t0.3768\t\t1.4962\t\t1.3209\t\t55.74\n",
      "3\t\t196\t\t0.5024\t\t1.4256\t\t1.0086\t\t65.75\n",
      "4\t\t196\t\t0.6305\t\t1.3725\t\t0.9878\t\t67.63\n",
      "5\t\t196\t\t0.7559\t\t1.3373\t\t1.0689\t\t65.11\n",
      "6\t\t196\t\t0.8806\t\t1.3103\t\t0.7507\t\t76.87\n",
      "7\t\t196\t\t1.0046\t\t1.2738\t\t0.7965\t\t75.49\n",
      "8\t\t196\t\t1.1313\t\t1.2532\t\t0.689\t\t79.0\n",
      "9\t\t196\t\t1.2553\t\t1.2293\t\t0.617\t\t81.61\n",
      "10\t\t196\t\t1.3789\t\t1.2167\t\t0.6576\t\t79.57\n",
      "11\t\t196\t\t1.5043\t\t1.2028\t\t0.6149\t\t81.7\n",
      "12\t\t196\t\t1.6285\t\t1.1818\t\t0.8252\t\t74.05\n",
      "13\t\t196\t\t1.7544\t\t1.175\t\t0.5709\t\t82.91\n",
      "14\t\t196\t\t1.8791\t\t1.1699\t\t0.5321\t\t84.43\n",
      "15\t\t196\t\t2.0042\t\t1.157\t\t0.4613\t\t86.4\n",
      "16\t\t196\t\t2.1303\t\t1.1429\t\t0.4744\t\t86.9\n",
      "17\t\t196\t\t2.2565\t\t1.1139\t\t0.4541\t\t87.53\n",
      "18\t\t196\t\t2.3818\t\t1.1013\t\t0.6191\t\t81.77\n",
      "19\t\t196\t\t2.5072\t\t1.0817\t\t0.3956\t\t89.94\n",
      "20\t\t196\t\t2.6331\t\t1.0713\t\t0.386\t\t90.2\n",
      "21\t\t196\t\t2.7599\t\t1.0571\t\t0.3731\t\t90.05\n",
      "22\t\t196\t\t2.8857\t\t1.0488\t\t0.3955\t\t89.6\n",
      "23\t\t196\t\t3.0117\t\t1.0311\t\t0.3502\t\t90.88\n",
      "24\t\t196\t\t3.1377\t\t1.0229\t\t0.3197\t\t91.98\n",
      "25\t\t196\t\t3.2641\t\t1.0075\t\t0.3251\t\t92.27\n",
      "26\t\t196\t\t3.389\t\t0.9933\t\t0.3298\t\t91.98\n",
      "27\t\t196\t\t3.5143\t\t0.982\t\t0.2852\t\t92.97\n",
      "28\t\t196\t\t3.6405\t\t0.971\t\t0.2841\t\t93.01\n",
      "29\t\t196\t\t3.7659\t\t0.9599\t\t0.2742\t\t93.53\n",
      "30\t\t196\t\t3.8919\t\t0.9553\t\t0.2726\t\t93.69\n",
      "31\t\t196\t\t4.0175\t\t0.9492\t\t0.2683\t\t93.62\n",
      "32\t\t196\t\t4.1431\t\t0.9452\t\t0.2672\t\t93.66\n",
      "33\t\t196\t\t4.2685\t\t0.9465\t\t0.2666\t\t93.72\n",
      "34\t\t196\t\t4.3931\t\t0.947\t\t0.2677\t\t93.81\n"
     ]
    }
   ],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "model = network_to_half(model)\n",
    "\n",
    "# args.full_precision = False\n",
    "# if not args.full_precision: model = network_to_half(model)\n",
    "# if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# # TESTING\n",
    "# args.full_precision = True\n",
    "# args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "# global model_params, master_params\n",
    "# if args.full_precision: master_params = model.parameters()\n",
    "# else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "# criterion = F.cross_entropy\n",
    "criterion = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False)).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "mixup_dl = MixUpDataLoader(trn_loader, 1)\n",
    "\n",
    "print(args)\n",
    "args.weight_decay = 1e-4\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "#     trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    trn_top1, trn_loss = train(mixup_dl, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "model = network_to_half(model)\n",
    "\n",
    "# args.full_precision = False\n",
    "# if not args.full_precision: model = network_to_half(model)\n",
    "# if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# # TESTING\n",
    "# args.full_precision = True\n",
    "# args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "# global model_params, master_params\n",
    "# if args.full_precision: master_params = model.parameters()\n",
    "# else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "# criterion = F.cross_entropy\n",
    "criterion = MixUpLoss(partial(nn.CrossEntropyLoss, reduce=False)).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "mixup_dl = MixUpDataLoader(trn_loader, .4)\n",
    "\n",
    "print(args)\n",
    "args.weight_decay = 5e-4\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "#     trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    trn_top1, trn_loss = train(mixup_dl, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
