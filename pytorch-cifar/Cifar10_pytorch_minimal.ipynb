{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse, os, shutil, time, warnings\n",
    "\n",
    "from fp16util import *\n",
    "from resnet import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--phases', default='[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]', type=str,\n",
    "                    help='Should be a string formatted like this: [(start_lr,end_lr,num_epochs),(phase2...)]')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Verbose logging')\n",
    "#     parser.add_argument('--init-bn0', action='store_true', help='Intialize running batch norm mean to 0')\n",
    "    parser.add_argument('--print-freq', '-p', default=200, type=int,\n",
    "                        metavar='N', help='print every this many steps (default: 5)')\n",
    "#     parser.add_argument('--no-bn-wd', action='store_true', help='Remove batch norm from weight decay')\n",
    "    parser.add_argument('--full-precision', action='store_true', help='Run model full precision mode. Default fp16')\n",
    "    parser.add_argument('--loss-scale', type=float, default=512,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--world-size', default=-1, type=int, \n",
    "                        help='total number of processes (machines*gpus)')\n",
    "    parser.add_argument('--scale-lr', type=float, default=1, help='You should learning rate propotionally to world size')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input = [\n",
    "    str(Path.home()/'data/cifar10/'),\n",
    "    '--phases', '[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]',\n",
    "#     '--phases', '[(0,2e-1,16),(2e-1,1e-2,16),(1e-2,0,5)]'\n",
    "#     '--full-precision'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "global args\n",
    "args = get_parser().parse_args(args_input)\n",
    "if args.full_precision: args.loss_scale = 1\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlock(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "        \n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "        \n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img, p=4, padding_mode='reflect'):\n",
    "    return Image.fromarray(np.pad(np.asarray(img), ((p, p), (p, p), (0, 0)), padding_mode))\n",
    "\n",
    "def torch_loader(data_path, size, bs, val_bs=None):\n",
    "    data_path = Path(data_path)\n",
    "    os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    # Data loading code\n",
    "    tfms = [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703,0.24349,0.26159))]\n",
    "    train_tfms = transforms.Compose([\n",
    "        pad, # TODO: use `padding` rather than assuming 4\n",
    "        transforms.RandomCrop(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tfms)\n",
    "    val_tfms = transforms.Compose(tfms)\n",
    "\n",
    "    download = (args.local_rank==0) and (data_path/'train').exists()\n",
    "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=download, transform=train_tfms)\n",
    "    val_dataset  = datasets.CIFAR10(root=data_path, train=False, download=download, transform=val_tfms)\n",
    "\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    # val_sampler = (torch.utils.data.distributed.DistributedSampler(val_dataset) if args.distributed else None)\n",
    "    val_sampler = None\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True,\n",
    "        sampler=val_sampler)\n",
    "    \n",
    "    train_loader = DataPrefetcher(train_loader)\n",
    "    val_loader = DataPrefetcher(val_loader)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None):\n",
    "        self.loader = loader\n",
    "        self.dataset = loader.dataset\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.stop_after = stop_after\n",
    "        self.next_input = None\n",
    "        self.next_target = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler():\n",
    "    def __init__(self, optimizer, phases=[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_lr = None\n",
    "        self.phases = phases\n",
    "        self.tot_epochs = sum([p[2] for p in phases])\n",
    "\n",
    "    def linear_lr(self, start_lr, end_lr, epoch_curr, batch_curr, epoch_tot, batch_tot):\n",
    "        if args.scale_lr != 1:\n",
    "            start_lr *= args.scale_lr\n",
    "            end_lr *= args.scale_lr\n",
    "        step_tot = epoch_tot * batch_tot\n",
    "        step_curr = epoch_curr * batch_tot + batch_curr\n",
    "        step_size = (end_lr - start_lr)/step_tot\n",
    "        return start_lr + step_curr * step_size\n",
    "    \n",
    "    def get_current_phase(self, epoch):\n",
    "        epoch_accum = 0\n",
    "        for phase in self.phases:\n",
    "            start_lr,end_lr,num_epochs = phase\n",
    "            if epoch <= epoch_accum+num_epochs: return start_lr, end_lr, num_epochs, epoch - epoch_accum\n",
    "            epoch_accum += num_epochs\n",
    "        raise Exception('Epoch out of range')\n",
    "            \n",
    "    def get_lr(self, epoch, batch_curr, batch_tot):\n",
    "        start_lr, end_lr, num_epochs, relative_epoch = self.get_current_phase(epoch)\n",
    "        return self.linear_lr(start_lr, end_lr, relative_epoch, batch_curr, num_epochs, batch_tot)\n",
    "\n",
    "    def update_lr(self, epoch, batch_num, batch_tot):\n",
    "        lr = self.get_lr(epoch, batch_num, batch_tot)\n",
    "        if args.verbose and (self.current_lr != lr) and ((batch_num == 1) or (batch_num == batch_tot)): \n",
    "            print(f'Changing LR from {self.current_lr} to {lr}')\n",
    "\n",
    "        self.current_lr = lr\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            lr_old = param_group['lr'] or lr\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if isinstance(t, float): return t\n",
    "    if isinstance(t, int): return t\n",
    "    if hasattr(t, 'item'): return t.item()\n",
    "    else: return t[0]\n",
    "\n",
    "def train(trn_loader, model, criterion, optimizer, scheduler, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    st = time.time()\n",
    "    trn_len = len(trn_loader)\n",
    "\n",
    "    # print('Begin training loop:', st)\n",
    "    for i,(input,target) in enumerate(trn_loader):\n",
    "        input = input.half()\n",
    "        batch_size = input.size(0)\n",
    "        batch_num = i+1\n",
    "        \n",
    "        # measure data loading time\n",
    "        scheduler.update_lr(epoch, i, trn_len)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        if args.distributed:\n",
    "            # Must keep track of global batch size, since not all machines are guaranteed equal batches at the end of an epoch\n",
    "            corr1 = correct(output.data, target)[0]\n",
    "            metrics = torch.tensor([batch_size, loss, corr1]).float().cuda()\n",
    "            batch_total, reduced_loss, corr1 = sum_tensor(metrics)\n",
    "            reduced_loss = reduced_loss/dist.get_world_size()\n",
    "            prec1 = corr1*(100.0/batch_total)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            batch_total = input.size(0)\n",
    "            prec1 = accuracy(output.data, target)[0] # measure accuracy and record loss\n",
    "        losses.update(to_python_float(reduced_loss), to_python_float(batch_total))\n",
    "        top1.update(to_python_float(prec1), to_python_float(batch_total))\n",
    "\n",
    "#         loss = loss*args.loss_scale\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "#         if args.full_precision:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         else:\n",
    "#             model.zero_grad()\n",
    "#             loss.backward()\n",
    "#             model_grads_to_master_grads(model_params, master_params)\n",
    "#             for param in master_params:\n",
    "#                 param.grad.data = param.grad.data/args.loss_scale\n",
    "#             optimizer.step()\n",
    "#             master_params_to_model_params(model_params, master_params)\n",
    "#             torch.cuda.synchronize()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==trn_len)\n",
    "        if should_print: log_batch(epoch, batch_num, trn_len, batch_time, losses, top1)\n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    val_len = len(val_loader)\n",
    "\n",
    "    for i,(input,target) in enumerate(val_loader):\n",
    "        input = input.half()\n",
    "        batch_num = i+1\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target).data\n",
    "        batch_total = input.size(0)\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "            \n",
    "        losses.update(to_python_float(loss), batch_total)\n",
    "        top1.update(to_python_float(prec1), batch_total)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        should_print = (batch_num%args.print_freq == 0) or (batch_num==val_len)\n",
    "        if should_print: log_batch(epoch, batch_num, val_len, batch_time, losses, top1)\n",
    "            \n",
    "    return top1.avg, losses.avg\n",
    "\n",
    "def log_batch(epoch, batch_num, batch_len, batch_time, loss, top1):\n",
    "    if args.local_rank==0 and args.verbose:\n",
    "        output = ('Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})').format(\n",
    "                epoch, batch_num, batch_len, batch_time=batch_time, loss=loss, top1=top1)\n",
    "        print(output)\n",
    "        with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "            f.write(output + '\\n')\n",
    "            \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = self.avg = self.sum = self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    corrrect_ks = correct(output, target, topk)\n",
    "    batch_size = target.size(0)\n",
    "    return [correct_k.float().mul_(100.0 / batch_size) for correct_k in corrrect_ks]\n",
    "\n",
    "def correct(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sum_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    return rt\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= args.world_size\n",
    "    return rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=False, local_rank=0, loss_scale=512, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0005, workers=8, world_size=-1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Namespace(batch_size=256, data='/home/paperspace/data/cifar10', dist_backend='nccl', dist_url='env://', distributed=False, full_precision=False, local_rank=0, loss_scale=512, momentum=0.9, phases='[(0,2e-1,15),(2e-1,1e-2,15),(1e-2,0,5)]', print_freq=200, save_dir=PosixPath('/home/paperspace/cluster/pytorch-cifar'), scale_lr=1, verbose=False, weight_decay=0.0005, workers=8, world_size=-1)\n",
      "\n",
      "\n",
      "\n",
      "epoch\t\tnum_batch\ttime (min)\ttrn_loss\tval_loss\taccuracy\n",
      "0\t\t196\t\t0.1595\t\t1.7201\t\t1.6815\t\t46.62\n",
      "1\t\t196\t\t0.3187\t\t1.0419\t\t1.1911\t\t59.69\n",
      "2\t\t196\t\t0.477\t\t0.7846\t\t1.1843\t\t62.43\n",
      "3\t\t196\t\t0.6381\t\t0.6608\t\t0.8821\t\t70.6\n",
      "4\t\t196\t\t0.7967\t\t0.5748\t\t0.8626\t\t71.66\n",
      "5\t\t196\t\t0.9572\t\t0.5254\t\t1.0739\t\t68.07\n",
      "6\t\t196\t\t1.1172\t\t0.478\t\t0.6915\t\t77.0\n",
      "7\t\t196\t\t1.2777\t\t0.446\t\t0.5284\t\t82.0\n",
      "8\t\t196\t\t1.4372\t\t0.4131\t\t1.0426\t\t68.95\n",
      "9\t\t196\t\t1.5961\t\t0.3984\t\t0.6383\t\t79.6\n",
      "10\t\t196\t\t1.7555\t\t0.3828\t\t0.5702\t\t80.72\n",
      "11\t\t196\t\t1.9159\t\t0.3842\t\t0.7105\t\t76.79\n",
      "12\t\t196\t\t2.0766\t\t0.3811\t\t1.0458\t\t71.3\n",
      "13\t\t196\t\t2.2361\t\t0.3834\t\t0.8059\t\t75.11\n",
      "14\t\t196\t\t2.3958\t\t0.38\t\t0.5796\t\t80.42\n",
      "15\t\t196\t\t2.5558\t\t0.3832\t\t0.703\t\t77.83\n",
      "16\t\t196\t\t2.7151\t\t0.3469\t\t0.7725\t\t75.8\n",
      "17\t\t196\t\t2.8748\t\t0.3208\t\t0.9043\t\t73.13\n",
      "18\t\t196\t\t3.0339\t\t0.305\t\t0.5072\t\t83.19\n",
      "19\t\t196\t\t3.1935\t\t0.2863\t\t0.8587\t\t75.72\n",
      "20\t\t196\t\t3.3521\t\t0.2712\t\t0.468\t\t85.0\n",
      "21\t\t196\t\t3.5102\t\t0.2556\t\t0.4629\t\t85.01\n",
      "22\t\t196\t\t3.6696\t\t0.2294\t\t0.3514\t\t88.27\n",
      "23\t\t196\t\t3.8284\t\t0.2108\t\t0.3277\t\t89.04\n",
      "24\t\t196\t\t3.9867\t\t0.1881\t\t0.4241\t\t86.11\n",
      "25\t\t196\t\t4.1454\t\t0.1653\t\t0.2884\t\t90.21\n",
      "26\t\t196\t\t4.3045\t\t0.1396\t\t0.2642\t\t91.31\n",
      "27\t\t196\t\t4.4639\t\t0.1124\t\t0.2808\t\t91.23\n",
      "28\t\t196\t\t4.6236\t\t0.0847\t\t0.2194\t\t93.1\n",
      "29\t\t196\t\t4.7826\t\t0.0583\t\t0.2081\t\t93.64\n",
      "30\t\t196\t\t4.9437\t\t0.0417\t\t0.1982\t\t93.86\n",
      "31\t\t196\t\t5.1033\t\t0.0389\t\t0.1987\t\t93.83\n",
      "32\t\t196\t\t5.2628\t\t0.0341\t\t0.1973\t\t94.07\n",
      "33\t\t196\t\t5.4218\t\t0.0304\t\t0.1967\t\t94.06\n",
      "34\t\t196\t\t5.5807\t\t0.0284\t\t0.1958\t\t94.08\n"
     ]
    }
   ],
   "source": [
    "if args.distributed:\n",
    "    print('Distributed: initializing process group')\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size)\n",
    "    assert(args.world_size == dist.get_world_size())\n",
    "    print(\"Distributed: success (%d/%d)\"%(args.local_rank, args.world_size))\n",
    "\n",
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "# model = network_to_half(model)\n",
    "model = model.half()\n",
    "\n",
    "# args.full_precision = False\n",
    "# if not args.full_precision: model = network_to_half(model)\n",
    "# if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "# # TESTING\n",
    "# args.full_precision = True\n",
    "# args.loss_scale = 1\n",
    "\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help accuracy\n",
    "# global model_params, master_params\n",
    "# if args.full_precision: master_params = model.parameters()\n",
    "# else: model_params, master_params = prep_param_lists(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0, nesterov=True, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer, phases=eval(args.phases))\n",
    "\n",
    "\n",
    "sz = 32\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "\n",
    "print(args)\n",
    "print('\\n\\n')\n",
    "print(\"epoch\\t\\tnum_batch\\ttime (min)\\ttrn_loss\\tval_loss\\taccuracy\")\n",
    "start_time = datetime.now() # Loading start to after everything is loaded\n",
    "for epoch in range(scheduler.tot_epochs):\n",
    "    trn_top1, trn_loss = train(trn_loader, model, criterion, optimizer, scheduler, epoch)\n",
    "    val_top1, val_loss = validate(val_loader, model, criterion, epoch, start_time)\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    minutes = float(time_diff.total_seconds() / 60.0)\n",
    "    # epoch   time   trn_loss   val_loss   accuracy     \n",
    "    metrics = [str(round(i, 4)) for i in [epoch, len(trn_loader), minutes, trn_loss, val_loss, val_top1]]\n",
    "    print('\\t\\t'.join(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import Learner, TrainingPhase, ModelData, accuracy, DecayType\n",
    "from functools import partial\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "model = model.cuda()\n",
    "model = network_to_half(model)\n",
    "\n",
    "# AS: todo: don't copy over weights as it seems to help performance\n",
    "\n",
    "wd=5e-4\n",
    "lr=1e-1\n",
    "momentum = 0.9\n",
    "# learn.clip = 1e-1\n",
    "bs = 256\n",
    "lrs = (0, 2e-1, 1e-2, 0)\n",
    "sz=32\n",
    "\n",
    "\n",
    "# data = torch_loader(args.data, sz, bs, bs*2)\n",
    "\n",
    "trn_loader, val_loader = torch_loader(args.data, sz, args.batch_size, args.batch_size*2)\n",
    "data = ModelData(args.data, trn_loader, val_loader)\n",
    "\n",
    "\n",
    "learn = Learner.from_model_data(model, data)\n",
    "learn.half()\n",
    "learn.crit = F.cross_entropy\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(torch.optim.SGD, nesterov=True, momentum=0.9)\n",
    "def_phase = {'opt_fn':learn.opt_fn, 'wds':wd, 'momentum':0.9}\n",
    "\n",
    "phases = [\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[:2], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=15, lr=lrs[1:3], lr_decay=DecayType.LINEAR),\n",
    "    TrainingPhase(**def_phase, epochs=5, lr=lrs[-2:], lr_decay=DecayType.LINEAR),\n",
    "]\n",
    "\n",
    "learn.fit_opt_sched(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6.802721088435374e-05,\n",
       " 0.00013605442176870748,\n",
       " 0.00020408163265306126,\n",
       " 0.00027210884353741496,\n",
       " 0.00034013605442176874,\n",
       " 0.0004081632653061225,\n",
       " 0.00047619047619047624,\n",
       " 0.0005442176870748299,\n",
       " 0.0006122448979591836,\n",
       " 0.0006802721088435375,\n",
       " 0.0007482993197278912,\n",
       " 0.000816326530612245,\n",
       " 0.0008843537414965988,\n",
       " 0.0009523809523809525,\n",
       " 0.001020408163265306,\n",
       " 0.0010884353741496598,\n",
       " 0.0011564625850340137,\n",
       " 0.0012244897959183673,\n",
       " 0.0012925170068027211,\n",
       " 0.001360544217687075,\n",
       " 0.0014285714285714286,\n",
       " 0.0014965986394557824,\n",
       " 0.0015646258503401362,\n",
       " 0.00163265306122449,\n",
       " 0.0017006802721088437,\n",
       " 0.0017687074829931975,\n",
       " 0.0018367346938775514,\n",
       " 0.001904761904761905,\n",
       " 0.0019727891156462586,\n",
       " 0.002040816326530612,\n",
       " 0.002108843537414966,\n",
       " 0.0021768707482993197,\n",
       " 0.0022448979591836735,\n",
       " 0.0023129251700680273,\n",
       " 0.002380952380952381,\n",
       " 0.0024489795918367346,\n",
       " 0.0025170068027210884,\n",
       " 0.0025850340136054422,\n",
       " 0.002653061224489796,\n",
       " 0.00272108843537415,\n",
       " 0.0027891156462585033,\n",
       " 0.002857142857142857,\n",
       " 0.002925170068027211,\n",
       " 0.002993197278911565,\n",
       " 0.0030612244897959186,\n",
       " 0.0031292517006802725,\n",
       " 0.003197278911564626,\n",
       " 0.00326530612244898,\n",
       " 0.0033333333333333335,\n",
       " 0.0034013605442176874,\n",
       " 0.0034693877551020408,\n",
       " 0.003537414965986395,\n",
       " 0.0036054421768707485,\n",
       " 0.0036734693877551027,\n",
       " 0.003741496598639456,\n",
       " 0.00380952380952381,\n",
       " 0.0038775510204081634,\n",
       " 0.003945578231292517,\n",
       " 0.004013605442176871,\n",
       " 0.004081632653061224,\n",
       " 0.004149659863945578,\n",
       " 0.004217687074829932,\n",
       " 0.004285714285714286,\n",
       " 0.004353741496598639,\n",
       " 0.004421768707482994,\n",
       " 0.004489795918367347,\n",
       " 0.004557823129251701,\n",
       " 0.004625850340136055,\n",
       " 0.004693877551020409,\n",
       " 0.004761904761904762,\n",
       " 0.004829931972789116,\n",
       " 0.004897959183673469,\n",
       " 0.004965986394557823,\n",
       " 0.005034013605442177,\n",
       " 0.005102040816326531,\n",
       " 0.0051700680272108845,\n",
       " 0.005238095238095239,\n",
       " 0.005306122448979592,\n",
       " 0.005374149659863946,\n",
       " 0.0054421768707483,\n",
       " 0.005510204081632653,\n",
       " 0.005578231292517007,\n",
       " 0.005646258503401361,\n",
       " 0.005714285714285714,\n",
       " 0.0057823129251700685,\n",
       " 0.005850340136054422,\n",
       " 0.005918367346938776,\n",
       " 0.00598639455782313,\n",
       " 0.006054421768707484,\n",
       " 0.006122448979591837,\n",
       " 0.006190476190476191,\n",
       " 0.006258503401360545,\n",
       " 0.0063265306122448975,\n",
       " 0.006394557823129252,\n",
       " 0.006462585034013606,\n",
       " 0.00653061224489796,\n",
       " 0.006598639455782313,\n",
       " 0.006666666666666667,\n",
       " 0.006734693877551021,\n",
       " 0.006802721088435375,\n",
       " 0.006870748299319728,\n",
       " 0.0069387755102040816,\n",
       " 0.007006802721088436,\n",
       " 0.00707482993197279,\n",
       " 0.007142857142857143,\n",
       " 0.007210884353741497,\n",
       " 0.007278911564625851,\n",
       " 0.0073469387755102054,\n",
       " 0.007414965986394558,\n",
       " 0.007482993197278912,\n",
       " 0.007551020408163266,\n",
       " 0.00761904761904762,\n",
       " 0.0076870748299319724,\n",
       " 0.007755102040816327,\n",
       " 0.007823129251700681,\n",
       " 0.007891156462585034,\n",
       " 0.007959183673469388,\n",
       " 0.008027210884353741,\n",
       " 0.008095238095238096,\n",
       " 0.008163265306122448,\n",
       " 0.008231292517006803,\n",
       " 0.008299319727891157,\n",
       " 0.008367346938775512,\n",
       " 0.008435374149659863,\n",
       " 0.008503401360544218,\n",
       " 0.008571428571428572,\n",
       " 0.008639455782312925,\n",
       " 0.008707482993197279,\n",
       " 0.008775510204081632,\n",
       " 0.008843537414965987,\n",
       " 0.00891156462585034,\n",
       " 0.008979591836734694,\n",
       " 0.009047619047619047,\n",
       " 0.009115646258503403,\n",
       " 0.009183673469387756,\n",
       " 0.00925170068027211,\n",
       " 0.009319727891156463,\n",
       " 0.009387755102040818,\n",
       " 0.009455782312925171,\n",
       " 0.009523809523809525,\n",
       " 0.009591836734693878,\n",
       " 0.009659863945578231,\n",
       " 0.009727891156462587,\n",
       " 0.009795918367346938,\n",
       " 0.009863945578231293,\n",
       " 0.009931972789115647,\n",
       " 0.010000000000000002,\n",
       " 0.010068027210884354,\n",
       " 0.010136054421768709,\n",
       " 0.010204081632653062,\n",
       " 0.010272108843537416,\n",
       " 0.010340136054421769,\n",
       " 0.010408163265306122,\n",
       " 0.010476190476190477,\n",
       " 0.010544217687074831,\n",
       " 0.010612244897959184,\n",
       " 0.010680272108843538,\n",
       " 0.010748299319727893,\n",
       " 0.010816326530612246,\n",
       " 0.0108843537414966,\n",
       " 0.010952380952380953,\n",
       " 0.011020408163265306,\n",
       " 0.011088435374149662,\n",
       " 0.011156462585034013,\n",
       " 0.011224489795918368,\n",
       " 0.011292517006802722,\n",
       " 0.011360544217687077,\n",
       " 0.011428571428571429,\n",
       " 0.011496598639455784,\n",
       " 0.011564625850340137,\n",
       " 0.01163265306122449,\n",
       " 0.011700680272108844,\n",
       " 0.011768707482993197,\n",
       " 0.011836734693877552,\n",
       " 0.011904761904761904,\n",
       " 0.01197278911564626,\n",
       " 0.012040816326530613,\n",
       " 0.012108843537414968,\n",
       " 0.01217687074829932,\n",
       " 0.012244897959183675,\n",
       " 0.012312925170068028,\n",
       " 0.012380952380952381,\n",
       " 0.012448979591836735,\n",
       " 0.01251700680272109,\n",
       " 0.012585034013605443,\n",
       " 0.012653061224489795,\n",
       " 0.012721088435374152,\n",
       " 0.012789115646258504,\n",
       " 0.012857142857142857,\n",
       " 0.012925170068027212,\n",
       " 0.012993197278911565,\n",
       " 0.01306122448979592,\n",
       " 0.013129251700680272,\n",
       " 0.013197278911564626,\n",
       " 0.01326530612244898,\n",
       " 0.013333333333333334,\n",
       " 0.013401360544217686,\n",
       " 0.013469387755102043,\n",
       " 0.013537414965986394,\n",
       " 0.01360544217687075,\n",
       " 0.013673469387755103,\n",
       " 0.013741496598639456,\n",
       " 0.013809523809523811,\n",
       " 0.013877551020408163,\n",
       " 0.013945578231292517,\n",
       " 0.014013605442176872,\n",
       " 0.014081632653061225,\n",
       " 0.01414965986394558,\n",
       " 0.014217687074829934,\n",
       " 0.014285714285714285,\n",
       " 0.01435374149659864,\n",
       " 0.014421768707482994,\n",
       " 0.014489795918367347,\n",
       " 0.014557823129251702,\n",
       " 0.014625850340136054,\n",
       " 0.014693877551020411,\n",
       " 0.014761904761904763,\n",
       " 0.014829931972789116,\n",
       " 0.014897959183673471,\n",
       " 0.014965986394557824,\n",
       " 0.015034013605442176,\n",
       " 0.015102040816326531,\n",
       " 0.015170068027210885,\n",
       " 0.01523809523809524,\n",
       " 0.015306122448979593,\n",
       " 0.015374149659863945,\n",
       " 0.015442176870748302,\n",
       " 0.015510204081632653,\n",
       " 0.015578231292517007,\n",
       " 0.015646258503401362,\n",
       " 0.015714285714285715,\n",
       " 0.01578231292517007,\n",
       " 0.015850340136054422,\n",
       " 0.015918367346938776,\n",
       " 0.015986394557823132,\n",
       " 0.016054421768707482,\n",
       " 0.016122448979591836,\n",
       " 0.016190476190476193,\n",
       " 0.016258503401360546,\n",
       " 0.016326530612244896,\n",
       " 0.016394557823129253,\n",
       " 0.016462585034013606,\n",
       " 0.01653061224489796,\n",
       " 0.016598639455782313,\n",
       " 0.016666666666666666,\n",
       " 0.016734693877551023,\n",
       " 0.016802721088435373,\n",
       " 0.016870748299319727,\n",
       " 0.016938775510204084,\n",
       " 0.017006802721088437,\n",
       " 0.01707482993197279,\n",
       " 0.017142857142857144,\n",
       " 0.017210884353741497,\n",
       " 0.01727891156462585,\n",
       " 0.017346938775510204,\n",
       " 0.017414965986394557,\n",
       " 0.017482993197278914,\n",
       " 0.017551020408163264,\n",
       " 0.01761904761904762,\n",
       " 0.017687074829931974,\n",
       " 0.017755102040816328,\n",
       " 0.01782312925170068,\n",
       " 0.017891156462585035,\n",
       " 0.017959183673469388,\n",
       " 0.01802721088435374,\n",
       " 0.018095238095238095,\n",
       " 0.01816326530612245,\n",
       " 0.018231292517006805,\n",
       " 0.018299319727891155,\n",
       " 0.018367346938775512,\n",
       " 0.018435374149659865,\n",
       " 0.01850340136054422,\n",
       " 0.018571428571428572,\n",
       " 0.018639455782312925,\n",
       " 0.018707482993197282,\n",
       " 0.018775510204081636,\n",
       " 0.018843537414965986,\n",
       " 0.018911564625850343,\n",
       " 0.018979591836734696,\n",
       " 0.01904761904761905,\n",
       " 0.019115646258503403,\n",
       " 0.019183673469387756,\n",
       " 0.01925170068027211,\n",
       " 0.019319727891156463,\n",
       " 0.019387755102040816,\n",
       " 0.019455782312925173,\n",
       " 0.019523809523809527,\n",
       " 0.019591836734693877,\n",
       " 0.019659863945578233,\n",
       " 0.019727891156462587,\n",
       " 0.01979591836734694,\n",
       " 0.019863945578231294,\n",
       " 0.019931972789115647,\n",
       " 0.020000000000000004,\n",
       " 0.020068027210884354,\n",
       " 0.020136054421768707,\n",
       " 0.020204081632653064,\n",
       " 0.020272108843537417,\n",
       " 0.020340136054421767,\n",
       " 0.020408163265306124,\n",
       " 0.020476190476190478,\n",
       " 0.02054421768707483,\n",
       " 0.020612244897959184,\n",
       " 0.020680272108843538,\n",
       " 0.020748299319727895,\n",
       " 0.020816326530612245,\n",
       " 0.020884353741496598,\n",
       " 0.020952380952380955,\n",
       " 0.02102040816326531,\n",
       " 0.021088435374149662,\n",
       " 0.021156462585034015,\n",
       " 0.02122448979591837,\n",
       " 0.021292517006802722,\n",
       " 0.021360544217687075,\n",
       " 0.02142857142857143,\n",
       " 0.021496598639455786,\n",
       " 0.021564625850340136,\n",
       " 0.021632653061224492,\n",
       " 0.021700680272108846,\n",
       " 0.0217687074829932,\n",
       " 0.021836734693877553,\n",
       " 0.021904761904761906,\n",
       " 0.02197278911564626,\n",
       " 0.022040816326530613,\n",
       " 0.022108843537414966,\n",
       " 0.022176870748299323,\n",
       " 0.022244897959183677,\n",
       " 0.022312925170068026,\n",
       " 0.022380952380952383,\n",
       " 0.022448979591836737,\n",
       " 0.02251700680272109,\n",
       " 0.022585034013605444,\n",
       " 0.022653061224489797,\n",
       " 0.022721088435374154,\n",
       " 0.022789115646258504,\n",
       " 0.022857142857142857,\n",
       " 0.022925170068027214,\n",
       " 0.022993197278911567,\n",
       " 0.023061224489795917,\n",
       " 0.023129251700680274,\n",
       " 0.023197278911564628,\n",
       " 0.02326530612244898,\n",
       " 0.023333333333333334,\n",
       " 0.023401360544217688,\n",
       " 0.023469387755102045,\n",
       " 0.023537414965986395,\n",
       " 0.023605442176870748,\n",
       " 0.023673469387755105,\n",
       " 0.02374149659863946,\n",
       " 0.023809523809523808,\n",
       " 0.023877551020408165,\n",
       " 0.02394557823129252,\n",
       " 0.024013605442176872,\n",
       " 0.024081632653061225,\n",
       " 0.02414965986394558,\n",
       " 0.024217687074829936,\n",
       " 0.024285714285714285,\n",
       " 0.02435374149659864,\n",
       " 0.024421768707482996,\n",
       " 0.02448979591836735,\n",
       " 0.024557823129251703,\n",
       " 0.024625850340136056,\n",
       " 0.02469387755102041,\n",
       " 0.024761904761904763,\n",
       " 0.024829931972789116,\n",
       " 0.02489795918367347,\n",
       " 0.024965986394557826,\n",
       " 0.02503401360544218,\n",
       " 0.025102040816326533,\n",
       " 0.025170068027210887,\n",
       " 0.02523809523809524,\n",
       " 0.02530612244897959,\n",
       " 0.02537414965986395,\n",
       " 0.025442176870748304,\n",
       " 0.025510204081632654,\n",
       " 0.025578231292517007,\n",
       " 0.02564625850340136,\n",
       " 0.025714285714285714,\n",
       " 0.02578231292517007,\n",
       " 0.025850340136054424,\n",
       " 0.025918367346938778,\n",
       " 0.02598639455782313,\n",
       " 0.02605442176870748,\n",
       " 0.02612244897959184,\n",
       " 0.026190476190476195,\n",
       " 0.026258503401360545,\n",
       " 0.026326530612244898,\n",
       " 0.02639455782312925,\n",
       " 0.026462585034013608,\n",
       " 0.02653061224489796,\n",
       " 0.026598639455782315,\n",
       " 0.02666666666666667,\n",
       " 0.026734693877551022,\n",
       " 0.02680272108843537,\n",
       " 0.026870748299319732,\n",
       " 0.026938775510204085,\n",
       " 0.027006802721088435,\n",
       " 0.02707482993197279,\n",
       " 0.027142857142857142,\n",
       " 0.0272108843537415,\n",
       " 0.027278911564625852,\n",
       " 0.027346938775510206,\n",
       " 0.02741496598639456,\n",
       " 0.027482993197278913,\n",
       " 0.027551020408163263,\n",
       " 0.027619047619047623,\n",
       " 0.027687074829931976,\n",
       " 0.027755102040816326,\n",
       " 0.02782312925170068,\n",
       " 0.027891156462585033,\n",
       " 0.02795918367346939,\n",
       " 0.028027210884353743,\n",
       " 0.028095238095238097,\n",
       " 0.02816326530612245,\n",
       " 0.028231292517006804,\n",
       " 0.02829931972789116,\n",
       " 0.028367346938775514,\n",
       " 0.028435374149659867,\n",
       " 0.028503401360544217,\n",
       " 0.02857142857142857,\n",
       " 0.028639455782312924,\n",
       " 0.02870748299319728,\n",
       " 0.028775510204081634,\n",
       " 0.028843537414965988,\n",
       " 0.02891156462585034,\n",
       " 0.028979591836734694,\n",
       " 0.02904761904761905,\n",
       " 0.029115646258503405,\n",
       " 0.029183673469387758,\n",
       " 0.029251700680272108,\n",
       " 0.02931972789115646,\n",
       " 0.029387755102040822,\n",
       " 0.02945578231292517,\n",
       " 0.029523809523809525,\n",
       " 0.02959183673469388,\n",
       " 0.029659863945578232,\n",
       " 0.029727891156462585,\n",
       " 0.029795918367346942,\n",
       " 0.029863945578231296,\n",
       " 0.02993197278911565,\n",
       " 0.03,\n",
       " 0.030068027210884352,\n",
       " 0.030136054421768713,\n",
       " 0.030204081632653063,\n",
       " 0.030272108843537416,\n",
       " 0.03034013605442177,\n",
       " 0.030408163265306123,\n",
       " 0.03047619047619048,\n",
       " 0.030544217687074833,\n",
       " 0.030612244897959186,\n",
       " 0.03068027210884354,\n",
       " 0.03074829931972789,\n",
       " 0.030816326530612243,\n",
       " 0.030884353741496604,\n",
       " 0.030952380952380953,\n",
       " 0.031020408163265307,\n",
       " 0.03108843537414966,\n",
       " 0.031156462585034014,\n",
       " 0.03122448979591837,\n",
       " 0.031292517006802724,\n",
       " 0.031360544217687074,\n",
       " 0.03142857142857143,\n",
       " 0.03149659863945578,\n",
       " 0.03156462585034014,\n",
       " 0.031632653061224494,\n",
       " 0.031700680272108844,\n",
       " 0.0317687074829932,\n",
       " 0.03183673469387755,\n",
       " 0.0319047619047619,\n",
       " 0.031972789115646265,\n",
       " 0.032040816326530615,\n",
       " 0.032108843537414965,\n",
       " 0.03217687074829932,\n",
       " 0.03224489795918367,\n",
       " 0.03231292517006803,\n",
       " 0.032380952380952385,\n",
       " 0.032448979591836735,\n",
       " 0.03251700680272109,\n",
       " 0.03258503401360544,\n",
       " 0.03265306122448979,\n",
       " 0.032721088435374156,\n",
       " 0.032789115646258506,\n",
       " 0.032857142857142856,\n",
       " 0.03292517006802721,\n",
       " 0.03299319727891156,\n",
       " 0.03306122448979592,\n",
       " 0.033129251700680276,\n",
       " 0.033197278911564626,\n",
       " 0.03326530612244898,\n",
       " 0.03333333333333333,\n",
       " 0.03340136054421769,\n",
       " 0.03346938775510205,\n",
       " 0.0335374149659864,\n",
       " 0.033605442176870746,\n",
       " 0.0336734693877551,\n",
       " 0.03374149659863945,\n",
       " 0.03380952380952381,\n",
       " 0.03387755102040817,\n",
       " 0.03394557823129252,\n",
       " 0.034013605442176874,\n",
       " 0.034081632653061224,\n",
       " 0.03414965986394558,\n",
       " 0.03421768707482994,\n",
       " 0.03428571428571429,\n",
       " 0.03435374149659864,\n",
       " 0.034421768707482994,\n",
       " 0.03448979591836735,\n",
       " 0.0345578231292517,\n",
       " 0.03462585034013606,\n",
       " 0.03469387755102041,\n",
       " 0.034761904761904765,\n",
       " 0.034829931972789115,\n",
       " 0.03489795918367347,\n",
       " 0.03496598639455783,\n",
       " 0.03503401360544218,\n",
       " 0.03510204081632653,\n",
       " 0.035170068027210885,\n",
       " 0.03523809523809524,\n",
       " 0.03530612244897959,\n",
       " 0.03537414965986395,\n",
       " 0.0354421768707483,\n",
       " 0.035510204081632656,\n",
       " 0.035578231292517006,\n",
       " 0.03564625850340136,\n",
       " 0.03571428571428572,\n",
       " 0.03578231292517007,\n",
       " 0.03585034013605442,\n",
       " 0.035918367346938776,\n",
       " 0.03598639455782313,\n",
       " 0.03605442176870748,\n",
       " 0.03612244897959184,\n",
       " 0.03619047619047619,\n",
       " 0.036258503401360546,\n",
       " 0.0363265306122449,\n",
       " 0.03639455782312925,\n",
       " 0.03646258503401361,\n",
       " 0.03653061224489796,\n",
       " 0.03659863945578231,\n",
       " 0.03666666666666667,\n",
       " 0.036734693877551024,\n",
       " 0.036802721088435374,\n",
       " 0.03687074829931973,\n",
       " 0.03693877551020408,\n",
       " 0.03700680272108844,\n",
       " 0.037074829931972794,\n",
       " 0.037142857142857144,\n",
       " 0.0372108843537415,\n",
       " 0.03727891156462585,\n",
       " 0.0373469387755102,\n",
       " 0.037414965986394565,\n",
       " 0.037482993197278915,\n",
       " 0.03755102040816327,\n",
       " 0.03761904761904762,\n",
       " 0.03768707482993197,\n",
       " 0.03775510204081633,\n",
       " 0.037823129251700685,\n",
       " 0.037891156462585035,\n",
       " 0.03795918367346939,\n",
       " 0.03802721088435374,\n",
       " 0.0380952380952381,\n",
       " 0.038163265306122456,\n",
       " 0.038231292517006805,\n",
       " 0.03829931972789116,\n",
       " 0.03836734693877551,\n",
       " 0.03843537414965986,\n",
       " 0.03850340136054422,\n",
       " 0.038571428571428576,\n",
       " 0.038639455782312926,\n",
       " 0.03870748299319728,\n",
       " 0.03877551020408163,\n",
       " 0.03884353741496599,\n",
       " 0.038911564625850346,\n",
       " 0.038979591836734696,\n",
       " 0.03904761904761905,\n",
       " 0.0391156462585034,\n",
       " 0.03918367346938775,\n",
       " 0.03925170068027212,\n",
       " 0.03931972789115647,\n",
       " 0.03938775510204082,\n",
       " 0.039455782312925174,\n",
       " 0.039523809523809524,\n",
       " 0.03959183673469388,\n",
       " 0.03965986394557824,\n",
       " 0.03972789115646259,\n",
       " 0.039795918367346944,\n",
       " 0.039863945578231294,\n",
       " 0.039931972789115644,\n",
       " 0.04000000000000001,\n",
       " 0.04006802721088436,\n",
       " 0.04013605442176871,\n",
       " 0.040204081632653065,\n",
       " 0.040272108843537414,\n",
       " 0.04034013605442177,\n",
       " 0.04040816326530613,\n",
       " 0.04047619047619048,\n",
       " 0.040544217687074835,\n",
       " 0.040612244897959185,\n",
       " 0.040680272108843535,\n",
       " 0.0407482993197279,\n",
       " 0.04081632653061225,\n",
       " 0.0408843537414966,\n",
       " 0.040952380952380955,\n",
       " 0.041020408163265305,\n",
       " 0.04108843537414966,\n",
       " 0.04115646258503402,\n",
       " 0.04122448979591837,\n",
       " 0.041292517006802726,\n",
       " 0.041360544217687076,\n",
       " 0.04142857142857143,\n",
       " 0.04149659863945579,\n",
       " 0.04156462585034014,\n",
       " 0.04163265306122449,\n",
       " 0.041700680272108846,\n",
       " 0.041768707482993196,\n",
       " 0.04183673469387755,\n",
       " 0.04190476190476191,\n",
       " 0.04197278911564626,\n",
       " 0.04204081632653062,\n",
       " 0.04210884353741497,\n",
       " 0.042176870748299324,\n",
       " 0.04224489795918368,\n",
       " 0.04231292517006803,\n",
       " 0.04238095238095238,\n",
       " 0.04244897959183674,\n",
       " 0.04251700680272109,\n",
       " 0.042585034013605444,\n",
       " 0.0426530612244898,\n",
       " 0.04272108843537415,\n",
       " 0.04278911564625851,\n",
       " 0.04285714285714286,\n",
       " 0.042925170068027214,\n",
       " 0.04299319727891157,\n",
       " 0.04306122448979592,\n",
       " 0.04312925170068027,\n",
       " 0.04319727891156463,\n",
       " 0.043265306122448985,\n",
       " 0.043333333333333335,\n",
       " 0.04340136054421769,\n",
       " 0.04346938775510204,\n",
       " 0.0435374149659864,\n",
       " 0.04360544217687075,\n",
       " 0.043673469387755105,\n",
       " 0.04374149659863946,\n",
       " 0.04380952380952381,\n",
       " 0.04387755102040816,\n",
       " 0.04394557823129252,\n",
       " 0.044013605442176876,\n",
       " 0.044081632653061226,\n",
       " 0.04414965986394558,\n",
       " 0.04421768707482993,\n",
       " 0.04428571428571429,\n",
       " 0.044353741496598646,\n",
       " 0.044421768707482996,\n",
       " 0.04448979591836735,\n",
       " 0.0445578231292517,\n",
       " 0.04462585034013605,\n",
       " 0.04469387755102041,\n",
       " 0.04476190476190477,\n",
       " 0.04482993197278912,\n",
       " 0.04489795918367347,\n",
       " 0.04496598639455782,\n",
       " 0.04503401360544218,\n",
       " 0.04510204081632654,\n",
       " 0.04517006802721089,\n",
       " 0.045238095238095244,\n",
       " 0.045306122448979594,\n",
       " 0.045374149659863944,\n",
       " 0.04544217687074831,\n",
       " 0.04551020408163266,\n",
       " 0.04557823129251701,\n",
       " 0.045646258503401364,\n",
       " 0.045714285714285714,\n",
       " 0.04578231292517007,\n",
       " 0.04585034013605443,\n",
       " 0.04591836734693878,\n",
       " 0.045986394557823135,\n",
       " 0.046054421768707485,\n",
       " 0.046122448979591835,\n",
       " 0.0461904761904762,\n",
       " 0.04625850340136055,\n",
       " 0.0463265306122449,\n",
       " 0.046394557823129255,\n",
       " 0.046462585034013605,\n",
       " 0.04653061224489796,\n",
       " 0.04659863945578232,\n",
       " 0.04666666666666667,\n",
       " 0.046734693877551026,\n",
       " 0.046802721088435376,\n",
       " 0.046870748299319726,\n",
       " 0.04693877551020409,\n",
       " 0.04700680272108844,\n",
       " 0.04707482993197279,\n",
       " 0.047142857142857146,\n",
       " 0.047210884353741496,\n",
       " 0.04727891156462585,\n",
       " 0.04734693877551021,\n",
       " 0.04741496598639456,\n",
       " 0.04748299319727892,\n",
       " 0.047551020408163266,\n",
       " 0.047619047619047616,\n",
       " 0.04768707482993198,\n",
       " 0.04775510204081633,\n",
       " 0.04782312925170068,\n",
       " 0.04789115646258504,\n",
       " 0.04795918367346939,\n",
       " 0.048027210884353744,\n",
       " 0.0480952380952381,\n",
       " 0.04816326530612245,\n",
       " 0.04823129251700681,\n",
       " 0.04829931972789116,\n",
       " 0.048367346938775514,\n",
       " 0.04843537414965987,\n",
       " 0.04850340136054422,\n",
       " 0.04857142857142857,\n",
       " 0.04863945578231293,\n",
       " 0.04870748299319728,\n",
       " 0.048775510204081635,\n",
       " 0.04884353741496599,\n",
       " 0.04891156462585034,\n",
       " 0.0489795918367347,\n",
       " 0.04904761904761905,\n",
       " 0.049115646258503405,\n",
       " 0.04918367346938776,\n",
       " 0.04925170068027211,\n",
       " 0.04931972789115646,\n",
       " 0.04938775510204082,\n",
       " 0.049455782312925176,\n",
       " 0.049523809523809526,\n",
       " 0.04959183673469388,\n",
       " 0.04965986394557823,\n",
       " 0.04972789115646259,\n",
       " 0.04979591836734694,\n",
       " 0.049863945578231296,\n",
       " 0.04993197278911565,\n",
       " 0.05,\n",
       " 0.05006802721088436,\n",
       " 0.05013605442176871,\n",
       " 0.050204081632653066,\n",
       " 0.050272108843537416,\n",
       " 0.05034013605442177,\n",
       " 0.05040816326530613,\n",
       " 0.05047619047619048,\n",
       " 0.05054421768707484,\n",
       " 0.05061224489795918,\n",
       " 0.050680272108843544,\n",
       " 0.0507482993197279,\n",
       " 0.050816326530612244,\n",
       " 0.05088435374149661,\n",
       " 0.05095238095238095,\n",
       " 0.05102040816326531,\n",
       " 0.05108843537414966,\n",
       " 0.051156462585034014,\n",
       " 0.05122448979591837,\n",
       " 0.05129251700680272,\n",
       " 0.05136054421768708,\n",
       " 0.05142857142857143,\n",
       " 0.051496598639455785,\n",
       " 0.05156462585034014,\n",
       " 0.05163265306122449,\n",
       " 0.05170068027210885,\n",
       " 0.0517687074829932,\n",
       " 0.051836734693877555,\n",
       " 0.05190476190476191,\n",
       " 0.05197278911564626,\n",
       " 0.05204081632653062,\n",
       " 0.05210884353741496,\n",
       " 0.052176870748299325,\n",
       " 0.05224489795918368,\n",
       " 0.052312925170068025,\n",
       " 0.05238095238095239,\n",
       " 0.05244897959183673,\n",
       " 0.05251700680272109,\n",
       " 0.05258503401360545,\n",
       " 0.052653061224489796,\n",
       " 0.05272108843537415,\n",
       " 0.0527891156462585,\n",
       " 0.05285714285714286,\n",
       " 0.052925170068027216,\n",
       " 0.052993197278911566,\n",
       " 0.05306122448979592,\n",
       " 0.05312925170068027,\n",
       " 0.05319727891156463,\n",
       " 0.05326530612244898,\n",
       " 0.05333333333333334,\n",
       " 0.053401360544217694,\n",
       " 0.053469387755102044,\n",
       " 0.0535374149659864,\n",
       " 0.05360544217687074,\n",
       " 0.05367346938775511,\n",
       " 0.053741496598639464,\n",
       " 0.05380952380952381,\n",
       " 0.05387755102040817,\n",
       " 0.053945578231292514,\n",
       " 0.05401360544217687,\n",
       " 0.054081632653061235,\n",
       " 0.05414965986394558,\n",
       " 0.054217687074829934,\n",
       " 0.054285714285714284,\n",
       " 0.05435374149659864,\n",
       " 0.054421768707483,\n",
       " 0.05448979591836735,\n",
       " 0.054557823129251705,\n",
       " 0.054625850340136055,\n",
       " 0.05469387755102041,\n",
       " 0.05476190476190477,\n",
       " 0.05482993197278912,\n",
       " 0.054897959183673475,\n",
       " 0.054965986394557825,\n",
       " 0.05503401360544218,\n",
       " 0.055102040816326525,\n",
       " 0.05517006802721089,\n",
       " 0.055238095238095246,\n",
       " 0.05530612244897959,\n",
       " 0.05537414965986395,\n",
       " 0.055442176870748296,\n",
       " 0.05551020408163265,\n",
       " 0.055578231292517016,\n",
       " 0.05564625850340136,\n",
       " 0.055714285714285716,\n",
       " 0.055782312925170066,\n",
       " 0.05585034013605442,\n",
       " 0.05591836734693878,\n",
       " 0.05598639455782313,\n",
       " 0.05605442176870749,\n",
       " 0.05612244897959184,\n",
       " 0.05619047619047619,\n",
       " 0.05625850340136055,\n",
       " 0.0563265306122449,\n",
       " 0.05639455782312926,\n",
       " 0.05646258503401361,\n",
       " 0.056530612244897964,\n",
       " 0.05659863945578232,\n",
       " 0.05666666666666667,\n",
       " 0.05673469387755103,\n",
       " 0.05680272108843537,\n",
       " 0.056870748299319734,\n",
       " 0.05693877551020409,\n",
       " 0.057006802721088434,\n",
       " 0.0570748299319728,\n",
       " 0.05714285714285714,\n",
       " 0.0572108843537415,\n",
       " 0.05727891156462585,\n",
       " 0.057346938775510205,\n",
       " 0.05741496598639456,\n",
       " 0.05748299319727891,\n",
       " 0.05755102040816327,\n",
       " 0.05761904761904762,\n",
       " 0.057687074829931975,\n",
       " 0.05775510204081633,\n",
       " 0.05782312925170068,\n",
       " 0.05789115646258504,\n",
       " 0.05795918367346939,\n",
       " 0.058027210884353746,\n",
       " 0.0580952380952381,\n",
       " 0.05816326530612245,\n",
       " 0.05823129251700681,\n",
       " 0.05829931972789115,\n",
       " 0.058367346938775516,\n",
       " 0.05843537414965987,\n",
       " 0.058503401360544216,\n",
       " 0.05857142857142858,\n",
       " 0.05863945578231292,\n",
       " 0.05870748299319728,\n",
       " 0.058775510204081644,\n",
       " 0.058843537414965986,\n",
       " 0.05891156462585034,\n",
       " 0.05897959183673469,\n",
       " 0.05904761904761905,\n",
       " 0.0591156462585034,\n",
       " 0.05918367346938776,\n",
       " 0.059251700680272114,\n",
       " 0.059319727891156464,\n",
       " 0.05938775510204082,\n",
       " 0.05945578231292517,\n",
       " 0.05952380952380953,\n",
       " 0.059591836734693884,\n",
       " 0.059659863945578234,\n",
       " 0.05972789115646259,\n",
       " 0.059795918367346934,\n",
       " 0.0598639455782313,\n",
       " 0.059931972789115655,\n",
       " 0.06,\n",
       " 0.06006802721088436,\n",
       " 0.060136054421768705,\n",
       " 0.06020408163265306,\n",
       " 0.060272108843537425,\n",
       " 0.06034013605442177,\n",
       " 0.060408163265306125,\n",
       " 0.060476190476190475,\n",
       " 0.06054421768707483,\n",
       " 0.06061224489795919,\n",
       " 0.06068027210884354,\n",
       " 0.060748299319727896,\n",
       " 0.060816326530612246,\n",
       " 0.0608843537414966,\n",
       " 0.06095238095238096,\n",
       " 0.06102040816326531,\n",
       " 0.061088435374149666,\n",
       " 0.061156462585034016,\n",
       " 0.06122448979591837,\n",
       " 0.061292517006802716,\n",
       " 0.06136054421768708,\n",
       " 0.06142857142857144,\n",
       " 0.06149659863945578,\n",
       " 0.06156462585034014,\n",
       " 0.061632653061224486,\n",
       " 0.06170068027210884,\n",
       " 0.06176870748299321,\n",
       " 0.06183673469387755,\n",
       " 0.06190476190476191,\n",
       " 0.06197278911564626,\n",
       " 0.062040816326530614,\n",
       " 0.06210884353741497,\n",
       " 0.06217687074829932,\n",
       " 0.06224489795918368,\n",
       " 0.06231292517006803,\n",
       " 0.062380952380952384,\n",
       " 0.06244897959183674,\n",
       " 0.06251700680272108,\n",
       " 0.06258503401360545,\n",
       " 0.0626530612244898,\n",
       " 0.06272108843537415,\n",
       " 0.06278911564625851,\n",
       " 0.06285714285714286,\n",
       " 0.06292517006802721,\n",
       " 0.06299319727891156,\n",
       " 0.06306122448979593,\n",
       " 0.06312925170068028,\n",
       " 0.06319727891156462,\n",
       " 0.06326530612244899,\n",
       " 0.06333333333333334,\n",
       " 0.06340136054421769,\n",
       " 0.06346938775510204,\n",
       " 0.0635374149659864,\n",
       " 0.06360544217687075,\n",
       " 0.0636734693877551,\n",
       " 0.06374149659863947,\n",
       " 0.0638095238095238,\n",
       " 0.06387755102040817,\n",
       " 0.06394557823129253,\n",
       " 0.06401360544217687,\n",
       " 0.06408163265306123,\n",
       " 0.06414965986394558,\n",
       " 0.06421768707482993,\n",
       " 0.0642857142857143,\n",
       " 0.06435374149659864,\n",
       " 0.064421768707483,\n",
       " 0.06448979591836734,\n",
       " 0.0645578231292517,\n",
       " 0.06462585034013606,\n",
       " 0.0646938775510204,\n",
       " 0.06476190476190477,\n",
       " 0.06482993197278912,\n",
       " 0.06489795918367347,\n",
       " 0.06496598639455783,\n",
       " 0.06503401360544218,\n",
       " 0.06510204081632653,\n",
       " 0.06517006802721088,\n",
       " 0.06523809523809525,\n",
       " 0.06530612244897958,\n",
       " 0.06537414965986395,\n",
       " 0.06544217687074831,\n",
       " 0.06551020408163265,\n",
       " 0.06557823129251701,\n",
       " 0.06564625850340136,\n",
       " 0.06571428571428571,\n",
       " 0.06578231292517008,\n",
       " 0.06585034013605442,\n",
       " 0.06591836734693877,\n",
       " 0.06598639455782312,\n",
       " 0.06605442176870749,\n",
       " 0.06612244897959184,\n",
       " 0.06619047619047619,\n",
       " 0.06625850340136055,\n",
       " 0.0663265306122449,\n",
       " 0.06639455782312925,\n",
       " 0.06646258503401362,\n",
       " 0.06653061224489797,\n",
       " 0.06659863945578232,\n",
       " 0.06666666666666667,\n",
       " 0.06673469387755103,\n",
       " 0.06680272108843538,\n",
       " 0.06687074829931973,\n",
       " 0.0669387755102041,\n",
       " 0.06700680272108843,\n",
       " 0.0670748299319728,\n",
       " 0.06714285714285714,\n",
       " 0.06721088435374149,\n",
       " 0.06727891156462586,\n",
       " 0.0673469387755102,\n",
       " 0.06741496598639456,\n",
       " 0.0674829931972789,\n",
       " 0.06755102040816327,\n",
       " 0.06761904761904762,\n",
       " 0.06768707482993197,\n",
       " 0.06775510204081633,\n",
       " 0.06782312925170068,\n",
       " 0.06789115646258503,\n",
       " 0.0679591836734694,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.sched.lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
